<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Aba's Blog]]></title><description><![CDATA[Tech - Travels - Thoughts]]></description><link>https://aba-blog.xyz/</link><image><url>https://aba-blog.xyz/favicon.png</url><title>Aba&apos;s Blog</title><link>https://aba-blog.xyz/</link></image><generator>Ghost 4.32</generator><lastBuildDate>Sat, 29 Jan 2022 18:50:02 GMT</lastBuildDate><atom:link href="https://aba-blog.xyz/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[DNN → Chip Design [4/4: System-on-Chip]]]></title><description><![CDATA[<blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><p>After building and testing each module and combining them hierarchically, it is time to build an SoC around it and</p>]]></description><link>https://aba-blog.xyz/dnn-to-chip-4/</link><guid isPermaLink="false">61f5189b33068f34ce882ef5</guid><category><![CDATA[Technical Projects]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sat, 29 Jan 2022 10:37:39 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2022/01/sys-4.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><img src="https://aba-blog.xyz/content/images/2022/01/sys-4.jpg" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]"><p>After building and testing each module and combining them hierarchically, it is time to build an SoC around it and control it. I used a Xilinx Zynq Z706 development board with a Z-4045 chip, which has an ARM Cortex processor and a Kintex FPGA on the same silicon die.</p><p>The following is the overview of the design. Gray-colored modules are Xilinx IPs. Two soft DMAs pull input \(\hat{X}\) and weight \(\hat{K}\) from the off-chip DDR and feed as two AXI4-Streams, which are then synchronized by the input pipe and provided to the Kraken Engine. The output \(\hat{Y}\) is stored back into the DDR through another soft DMA. The three soft DMAs are controlled by commands issued by the ARM Cortex core, as dictated by the firmware which I then developed.</p><figure class="kg-card kg-image-card kg-width-wide"><img src="https://aba-blog.xyz/content/images/2022/01/sys-1.jpg" class="kg-image" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]" loading="lazy" width="1280" height="720" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/sys-1.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/sys-1.jpg 1000w, https://aba-blog.xyz/content/images/2022/01/sys-1.jpg 1280w" sizes="(min-width: 1200px) 1200px"></figure><!--kg-card-begin: markdown--><ol start="11">
<li><strong>SoC Block Design</strong>: Build FPGA projects with Vivado manually and synthesize</li>
<li><strong>Automation</strong>: TCL scripts to automate the project building and configuration</li>
<li><strong>C++ Firmware</strong>: To control the custom modules</li>
<li><strong>Hardware Verification</strong>: Test on FPGA, compare output to golden model</li>
<li>Repeat 11-14</li>
</ol>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">11. SoC Block Design</h2></div><p>I add my custom modules to a Vivado block design, add soft DMAs from the IP catalog, configure them, connect them to my main module, run block &amp; connection automation, copying down TCL commands at every step.</p><figure class="kg-card kg-image-card kg-width-full"><img src="https://aba-blog.xyz/content/images/2022/01/image-15.png" class="kg-image" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]" loading="lazy" width="1920" height="1042" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-15.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-15.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/image-15.png 1600w, https://aba-blog.xyz/content/images/2022/01/image-15.png 1920w"></figure><p></p><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">12. TCL Automation</h2></div><p>Xilinx Vivado projects are notoriously buggy. They crash once in a while and get corrupted. Vivado also auto-generates hundreds of small files, which contain absolute paths, and don&apos;t play well with a different Vivado version. Therefore, it is a bad idea to version control them. </p><p>The best practice is to script the project flow. Once I manually copy down the TCL commands, I change them into parameterized code. </p><figure class="kg-card kg-code-card"><pre><code class="language-tcl">set TUSER_WIDTH_MAXPOOL_IN     [expr $BITS_KW2      + $I_KW2]
set TUSER_WIDTH_LRELU_IN       [expr $BITS_KW       + $I_CLR]
set TUSER_CONV_DW_BASE         [expr 1 + $I_IS_BOTTOM_BLOCK ]
set TUSER_CONV_DW_IN           [expr $MEMBERS*$BITS_KW + $BITS_OUT_SHIFT + $BITS_MEMBERS + $TUSER_CONV_DW_BASE]
set TUSER_WIDTH_LRELU_FMA_1_IN [expr 1         + $I_IS_LRELU]
set TUSER_WIDTH_CONV_IN        [expr $I_IS_SUM_START     + 1]

set DEBUG_CONFIG_WIDTH_W_ROT   [expr 1 + 2*$BITS_KW2 + 3*($BITS_KH2      + $BITS_IM_CIN + $BITS_IM_COLS + $BITS_IM_BLOCKS)]
set DEBUG_CONFIG_WIDTH_IM_PIPE [expr 3 + 2 + $BITS_KH2      + 0]
set DEBUG_CONFIG_WIDTH_LRELU   [expr 3 + 4 + $BITS_FMA_2]
set DEBUG_CONFIG_WIDTH_MAXPOOL 1
set DEBUG_CONFIG_WIDTH         [expr $DEBUG_CONFIG_WIDTH_MAXPOOL + $DEBUG_CONFIG_WIDTH_LRELU + 2*$BITS_KH2      + $DEBUG_CONFIG_WIDTH_IM_PIPE + $DEBUG_CONFIG_WIDTH_W_ROT]

# **********    STORE PARAMS    *************
set file_param [open $RTL_DIR/include/params.v w]
if {$MAC_TYPE == &quot;XILINX&quot;} {
  puts $file_param &quot;`define MAC_XILINX 1&quot;
}
if {$REG_TYPE == &quot;ASIC&quot;} {
  puts $file_param &quot;`define ASIC_REG 1&quot;
}
puts $file_param &quot;
`define SRAM_TYPE   \&quot;$SRAM_TYPE\&quot;  
`define MAC_TYPE    \&quot;$MAC_TYPE\&quot;  
`define UNITS    $UNITS  
`define GROUPS   $GROUPS 
`define COPIES   $COPIES 
`define MEMBERS  $MEMBERS
`define DW_FACTOR_1 $DW_FACTOR_1
`define OUTPUT_MODE \&quot;$OUTPUT_MODE\&quot;
`define KSM_COMBS_EXPR $KSM_COMBS_EXPR
`define KS_COMBS_EXPR $KS_COMBS_EXPR</code></pre><figcaption>Part of the tcl script that calculates parameters and writes the params.v file</figcaption></figure><p>I then spend a couple of days debugging the TCL script to ensure it can reliably rebuild a project from scratch. These TCL scripts and the source verilog files are tracked by git.</p><figure class="kg-card kg-code-card"><pre><code class="language-tcl">create_bd_design &quot;sys&quot;

# ZYNQ IP
create_bd_cell -type ip -vlnv xilinx.com:ip:processing_system7:5.5 processing_system7_0
apply_bd_automation -rule xilinx.com:bd_rule:processing_system7 -config {make_external &quot;FIXED_IO, DDR&quot; apply_board_preset &quot;1&quot; Master &quot;Disable&quot; Slave &quot;Disable&quot; }  [get_bd_cells processing_system7_0]
set_property -dict [list CONFIG.PCW_FPGA0_PERIPHERAL_FREQMHZ $FREQ_LITE CONFIG.PCW_FPGA1_PERIPHERAL_FREQMHZ $FREQ_LOW CONFIG.PCW_FPGA2_PERIPHERAL_FREQMHZ $FREQ_HIGH CONFIG.PCW_EN_CLK2_PORT $FREQ_LITE CONFIG.PCW_USE_S_AXI_ACP {1} CONFIG.PCW_USE_DEFAULT_ACP_USER_VAL {1} CONFIG.PCW_USE_FABRIC_INTERRUPT {1} CONFIG.PCW_EN_CLK1_PORT {1} CONFIG.PCW_EN_CLK2_PORT {1} CONFIG.PCW_IRQ_F2P_INTR {1} CONFIG.PCW_QSPI_PERIPHERAL_ENABLE {0} CONFIG.PCW_SD0_PERIPHERAL_ENABLE {0} CONFIG.PCW_I2C0_PERIPHERAL_ENABLE {0} CONFIG.PCW_GPIO_MIO_GPIO_ENABLE {0}] [get_bd_cells processing_system7_0]

# Accelerator
create_bd_cell -type module -reference axis_accelerator axis_accelerator_0

# Weights &amp; out DMA
set IP_NAME &quot;dma_weights_im_out&quot;
create_bd_cell -type ip -vlnv xilinx.com:ip:axi_dma:7.1 $IP_NAME
set_property -dict [list CONFIG.c_include_sg {0} CONFIG.c_sg_length_width {26} CONFIG.c_m_axi_mm2s_data_width {32} CONFIG.c_m_axis_mm2s_tdata_width {32} CONFIG.c_mm2s_burst_size {8} CONFIG.c_sg_include_stscntrl_strm {0} CONFIG.c_include_mm2s_dre {1} CONFIG.c_m_axi_mm2s_data_width $S_WEIGHTS_WIDTH_LF CONFIG.c_m_axis_mm2s_tdata_width $S_WEIGHTS_WIDTH_LF CONFIG.c_m_axi_s2mm_data_width $M_DATA_WIDTH_LF CONFIG.c_s_axis_s2mm_tdata_width $M_DATA_WIDTH_LF CONFIG.c_include_s2mm_dre {1} CONFIG.c_s2mm_burst_size {16}] [get_bd_cells $IP_NAME]

# Im_in_1
set IP_NAME &quot;dma_im_in&quot;
create_bd_cell -type ip -vlnv xilinx.com:ip:axi_dma:7.1 $IP_NAME
set_property -dict [list CONFIG.c_include_sg {0} CONFIG.c_sg_length_width {26} CONFIG.c_m_axi_mm2s_data_width [expr $S_PIXELS_WIDTH_LF] CONFIG.c_m_axis_mm2s_tdata_width [expr $S_PIXELS_WIDTH_LF] CONFIG.c_include_mm2s_dre {1} CONFIG.c_mm2s_burst_size {64} CONFIG.c_include_s2mm {0}] [get_bd_cells $IP_NAME]

# # Interrupts
create_bd_cell -type ip -vlnv xilinx.com:ip:xlconcat:2.1 xlconcat_0
set_property -dict [list CONFIG.NUM_PORTS {4}] [get_bd_cells xlconcat_0]
connect_bd_net [get_bd_pins dma_im_in/mm2s_introut] [get_bd_pins xlconcat_0/In0]
connect_bd_net [get_bd_pins dma_weights_im_out/mm2s_introut] [get_bd_pins xlconcat_0/In2]
connect_bd_net [get_bd_pins dma_weights_im_out/s2mm_introut] [get_bd_pins xlconcat_0/In3]
connect_bd_net [get_bd_pins xlconcat_0/dout] [get_bd_pins processing_system7_0/IRQ_F2P]

# Engine connections
connect_bd_net [get_bd_pins processing_system7_0/FCLK_CLK1] [get_bd_pins axis_accelerator_0/aclk]
connect_bd_net [get_bd_pins processing_system7_0/FCLK_CLK2] [get_bd_pins axis_accelerator_0/hf_aclk]
connect_bd_intf_net [get_bd_intf_pins dma_im_in/M_AXIS_MM2S] [get_bd_intf_pins axis_accelerator_0/s_axis_pixels]
connect_bd_intf_net [get_bd_intf_pins dma_weights_im_out/M_AXIS_MM2S] [get_bd_intf_pins axis_accelerator_0/s_axis_weights]
switch $OUTPUT_MODE {
  &quot;CONV&quot;    {connect_bd_intf_net [get_bd_intf_pins dma_weights_im_out/S_AXIS_S2MM] [get_bd_intf_pins axis_accelerator_0/conv_dw2_lf_m_axis]}
  &quot;LRELU&quot;   {connect_bd_intf_net [get_bd_intf_pins dma_weights_im_out/S_AXIS_S2MM] [get_bd_intf_pins axis_accelerator_0/lrelu_dw_lf_m_axis]}
  &quot;MAXPOOL&quot; {connect_bd_intf_net [get_bd_intf_pins dma_weights_im_out/S_AXIS_S2MM] [get_bd_intf_pins axis_accelerator_0/max_dw2_lf_m_axis ]}
}

# AXI Lite
startgroup
apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Clk_slave {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Master {/processing_system7_0/M_AXI_GP0} Slave {/dma_im_in/S_AXI_LITE} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins dma_im_in/S_AXI_LITE]
apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Clk_slave {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK0 ($FREQ_LITE MHz)} Master {/processing_system7_0/M_AXI_GP0} Slave {/dma_weights_im_out/S_AXI_LITE} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins dma_weights_im_out/S_AXI_LITE]
endgroup

# AXI4
startgroup
apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_slave {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Master {/dma_weights_im_out/M_AXI_MM2S} Slave {/processing_system7_0/S_AXI_ACP} ddr_seg {Auto} intc_ip {New AXI Interconnect} master_apm {0}}  [get_bd_intf_pins processing_system7_0/S_AXI_ACP]
apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_slave {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Master {/dma_im_in/M_AXI_MM2S} Slave {/processing_system7_0/S_AXI_ACP} ddr_seg {Auto} intc_ip {/axi_mem_intercon} master_apm {0}}  [get_bd_intf_pins dma_im_in/M_AXI_MM2S]
apply_bd_automation -rule xilinx.com:bd_rule:axi4 -config { Clk_master {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_slave {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Clk_xbar {/processing_system7_0/FCLK_CLK1 ($FREQ_LOW MHz)} Master {/dma_weights_im_out/M_AXI_S2MM} Slave {/processing_system7_0/S_AXI_ACP} ddr_seg {Auto} intc_ip {/axi_mem_intercon} master_apm {0}}  [get_bd_intf_pins dma_weights_im_out/M_AXI_S2MM]
endgroup

# HF Reset
set IP_NAME &quot;reset_hf&quot;
create_bd_cell -type ip -vlnv xilinx.com:ip:proc_sys_reset:5.0 $IP_NAME
connect_bd_net [get_bd_pins processing_system7_0/FCLK_CLK2] [get_bd_pins $IP_NAME/slowest_sync_clk]
connect_bd_net [get_bd_pins processing_system7_0/FCLK_RESET0_N] [get_bd_pins $IP_NAME/ext_reset_in]
connect_bd_net [get_bd_pins $IP_NAME/peripheral_aresetn] [get_bd_pins axis_accelerator_0/hf_aresetn]

# LF Reset
# NOTE: axi_mem_intercon gets created after axi lite
connect_bd_net [get_bd_pins axis_accelerator_0/aresetn] [get_bd_pins axi_mem_intercon/ARESETN]

save_bd_design
validate_bd_design

generate_target all [get_files  $PROJ_FOLDER/$PROJ_NAME.srcs/sources_1/bd/sys/sys.bd]
make_wrapper -files [get_files $PROJ_FOLDER/$PROJ_NAME.srcs/sources_1/bd/sys/sys.bd] -top
add_files -norecurse $PROJ_FOLDER/$PROJ_NAME.gen/sources_1/bd/sys/hdl/sys_wrapper.v
set_property top sys_wrapper [current_fileset]</code></pre><figcaption>TCL file that builds the project.</figcaption></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">13. C++ Firmware</h2></div><p>I then write the C++ code to be run on the ARM processor, which instructs the DMA to pull data from memory and push it back. When multiple DMAs are involved, this is fairly tricky. Right after starting a DMA operation, the parameters for the next DMA iteration must be calculated in advance, to prevent stalling the DMA.</p><h3 id="131-oop-wrappers-for-dma-drivers">13.1. OOP Wrappers for DMA Drivers</h3><p>I find the C code provided by Xilinx a bit counterintuitive. Therefore, I have written an OOP wrapper for the Xilinx DMA, which is open-sourced here:</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://github.com/abarajithan11/zynq-oop-drivers"><div class="kg-bookmark-content"><div class="kg-bookmark-title">GitHub - abarajithan11/zynq-oop-drivers: Object oriented drivers for Xilinx IPs (DMA...) for ZYNQ PSoC</div><div class="kg-bookmark-description">Object oriented drivers for Xilinx IPs (DMA...) for ZYNQ PSoC - GitHub - abarajithan11/zynq-oop-drivers: Object oriented drivers for Xilinx IPs (DMA...) for ZYNQ PSoC</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://github.com/fluidicon.png" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]"><span class="kg-bookmark-author">GitHub</span><span class="kg-bookmark-publisher">abarajithan11</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://opengraph.githubassets.com/68b1de5a4adce9b41978d0009c3d32b23fa6e0edbafc33a9c62cd23da784f29a/abarajithan11/zynq-oop-drivers" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]"></div></a></figure><h3 id="132-oop-architecture-for-dnn-models-config-bits-in-c">13.2. OOP Architecture for DNN models &amp; config bits in C++</h3><p>The firmware needs to be flexible, such that I can create any DNN by chaining layer objects. For this, I write the layer class, with necessary features like extracting configuration bits and appending to data. This is basically like another golden model, but for the purposes of running in the ARM processor.</p><pre><code class="language-cpp">class Layer
{
public:
	 int idx, H_IN, W_IN, C_IN, C_OUT, KH_IN, KW_IN;
	 bool IS_NOT_MAX, IS_MAX, IS_LRELU;

	 Layer * PREV_P = nullptr;
	 Layer * NEXT_P = nullptr;

	 int BLOCKS, BLOCKS_PER_ARR;
	 u8 MAX_FACTOR, SUB_CORES, EFF_CORES, ITR, COUT_FPGA, COUT_VALID, COUT_INVALID;
	 u8 KW_PAD;

	 int OUT_W_IN, OUT_BLOCKS, OUT_MAX_FACTOR, OUT_BLOCKS_PER_ARR, OUT_KH;

	 int DATA_BEATS_PIXELS;
	 int BEATS_LRELU = 0;
	 int WORDS_PIXELS_PER_ARR;
	 int WORDS_WEIGHTS_PER_ITR, WORDS_WEIGHTS;

	 int WORDS_OUT_PER_TRANSFER, TRANSFERS_OUT_PER_ITR;
	 int WORDS_OUT_PER_TRANSFER_ARR [3];

	 chunk_s * input_chunk_p  = nullptr;
	 chunk_s * output_chunk_p = nullptr;
	 bool done_write = false;

	 Layer ( int idx,
			 int H_IN, int W_IN, int C_IN, int C_OUT,
			 int KH_IN, int KW_IN,
			 bool IS_NOT_MAX, bool IS_MAX, bool IS_LRELU):
					 idx    (idx),
					 H_IN   (H_IN),
					 W_IN   (W_IN),
					 C_IN   (C_IN),
					 C_OUT  (C_OUT),
					 KH_IN   (KH_IN),
					 KW_IN   (KW_IN),
					 IS_NOT_MAX(IS_NOT_MAX),
					 IS_MAX    (IS_MAX),
					 IS_LRELU  (IS_LRELU)
	 {
		 BLOCKS     = H_IN / UNITS;
		 MAX_FACTOR = IS_MAX ? 2 : 1;
		 BLOCKS_PER_ARR = BLOCKS / MAX_FACTOR;

		 KW_PAD = KW_IN - 2*IS_MAX;

		 SUB_CORES = MEMBERS / KW_IN;
		 EFF_CORES = COPIES * GROUPS * SUB_CORES / MAX_FACTOR;
		 ITR       = (int)(std::ceil((float)C_OUT / (float)EFF_CORES));
		 COUT_FPGA = EFF_CORES * ITR;

		 COUT_VALID = C_OUT % EFF_CORES;
		 COUT_VALID = (COUT_VALID == 0) ? EFF_CORES : COUT_VALID;

		 COUT_INVALID = EFF_CORES - COUT_VALID;

		 /* LRELU BEATS */

		 BEATS_LRELU += 1; //D
		 BEATS_LRELU += ceil(2.0/KW_IN); // A

		 for (int clr_i=0;  clr_i &lt; KW_IN/2+1; clr_i ++){
			 int clr = clr_i*2 +1;
			 for (int mtb=0;  mtb &lt; clr; mtb ++){
				 int bram_width = MEMBERS/clr;
				 int bram_size  = 2*SUB_CORES;
				 int BEATS_ij = ceil((float)bram_size/bram_width);

				 BEATS_LRELU += BEATS_ij;
			 }
		 }

		 DATA_BEATS_PIXELS = BLOCKS_PER_ARR * W_IN * C_IN;

		 WORDS_PIXELS_PER_ARR  =      DATA_BEATS_PIXELS  * UNITS_EDGES;
		 WORDS_WEIGHTS_PER_ITR = (S_WEIGHTS_WIDTH/8) + (BEATS_LRELU + C_IN*KH_IN) * COPIES * GROUPS * MEMBERS;
		 WORDS_WEIGHTS         = ITR * WORDS_WEIGHTS_PER_ITR;

		 if (IS_NOT_MAX &amp;&amp; IS_MAX)
		 {
			 WORDS_OUT_PER_TRANSFER_ARR[0] = SUB_CORES * COPIES * GROUPS * UNITS_EDGES;
			 WORDS_OUT_PER_TRANSFER_ARR[1] =             COPIES * GROUPS * UNITS_EDGES;
			 WORDS_OUT_PER_TRANSFER_ARR[2] =             COPIES * GROUPS * UNITS_EDGES / MAX_FACTOR;

			 TRANSFERS_OUT_PER_ITR = BLOCKS/MAX_FACTOR * W_IN/MAX_FACTOR * (1 + 2 * SUB_CORES);
		 }
		 else
		 {
			 WORDS_OUT_PER_TRANSFER = SUB_CORES * COPIES * GROUPS * UNITS_EDGES / MAX_FACTOR;
			 TRANSFERS_OUT_PER_ITR  = BLOCKS/MAX_FACTOR * W_IN/MAX_FACTOR;
		 }
	 };

	 void set_config()
	 {
		 input_chunk_p-&gt;data_p[0] = (s8)(IS_NOT_MAX);
		 input_chunk_p-&gt;data_p[1] = (s8)(IS_MAX);
		 input_chunk_p-&gt;data_p[2] = (s8)(IS_LRELU);
		 input_chunk_p-&gt;data_p[3] = (s8)(KH_IN/2);

#ifdef DEBUG
		 for (int i=4; i&lt;UNITS_EDGES; i++) input_chunk_p-&gt;data_p[i] = 0;
#endif
		 Xil_DCacheFlushRange((UINTPTR)input_chunk_p-&gt;data_p, UNITS_EDGES);
	 };

	 void set_out_params()
	 {
		 /* Next layer can be null (if this is last) or can have multiple next layers.
		  * We are interested in how to arrange the output values of this, to match the next
		  */

		 OUT_W_IN   = W_IN / MAX_FACTOR;
		 OUT_BLOCKS = (H_IN / MAX_FACTOR) / UNITS;

		 OUT_MAX_FACTOR     = (NEXT_P == nullptr) ? 1 : NEXT_P-&gt;MAX_FACTOR;
		 OUT_BLOCKS_PER_ARR = OUT_BLOCKS/OUT_MAX_FACTOR;

		 OUT_KH = (NEXT_P == nullptr) ? KH_IN : NEXT_P-&gt;KH_IN;
	 }

	 inline s8* get_input_pixels_base_p()
	 {
		 return (s8*)(input_chunk_p-&gt;data_p) + UNITS_EDGES;
	 }
	 inline s8* get_output_pixels_base_p()
	 {
		 return (s8*)(output_chunk_p-&gt;data_p) + UNITS_EDGES;
	 }
};</code></pre><pre><code class="language-cpp">auto build_yolo_mod()
{
	std::array&lt;Layer,21&gt; layers = {
		Layer(1,	H_RGB   ,W_RGB  ,    3,  32,   3,   3,false, true, true),
		Layer(2,	H_RGB/2 ,W_RGB/2,   32,  64,   3,   3,false, true, true),
		Layer(3,	H_RGB/4 ,W_RGB/4,   64, 128,   3,   3,true, false, true),
		Layer(4,	H_RGB/4 ,W_RGB/4,  128,  64,   1,   1,true, false, true),
		Layer(5,	H_RGB/4 ,W_RGB/4,   64, 128,   3,   3,false, true, true),
		Layer(6,	H_RGB/8 ,W_RGB/8,  128, 256,   3,   3,true, false, true),
		Layer(7,	H_RGB/8 ,W_RGB/8,  256, 128,   1,   1,true, false, true),
		Layer(8,	H_RGB/8 ,W_RGB/8,  128, 256,   3,   3,false, true, true),
		Layer(9,	H_RGB/16,W_RGB/16, 256, 512,   3,   3,true, false, true),
		Layer(10,	H_RGB/16,W_RGB/16, 512, 256,   1,   1,true, false, true),
		Layer(11,	H_RGB/16,W_RGB/16, 256, 512,   3,   3,true, false, true),
		Layer(12,	H_RGB/16,W_RGB/16, 512, 256,   1,   1,true, false, true),
		Layer(13,	H_RGB/16,W_RGB/16, 256, 512,   3,   3,false, true, true),
		Layer(14,	H_RGB/32,W_RGB/32, 512,1024,   3,   3,true, false, true),
		Layer(15,	H_RGB/32,W_RGB/32,1024, 512,   1,   1,true, false, true),
		Layer(16,	H_RGB/32,W_RGB/32, 512,1024,   3,   3,true, false, true),
		Layer(17,	H_RGB/32,W_RGB/32,  64, 128,1024, 512,true, false, true),
		Layer(18,	H_RGB/32,W_RGB/32,  64, 128, 512,1024,true, false, true),
		Layer(19,	H_RGB/32,W_RGB/32,1024,1024,   3,   3,true, false, true),
		Layer(20,	H_RGB/32,W_RGB/32,1024,1024,   3,   3,true, false, true),
		Layer(21,	H_RGB/32,W_RGB/32,1024,  45,   1,   1,true, false, false)
	};

	for (int i=0; i &lt; N_LAYERS; i++)
	{
		if (i!=0         ) layers[i].PREV_P = &amp;layers[i-1];
		if (i!=N_LAYERS-1) layers[i].NEXT_P = &amp;layers[i+1];
		layers[i].set_out_params();
	}
	return layers;
}</code></pre><h3 id="133-c-code-to-control-multiple-dmas-effectively">13.3. C++ Code to control multiple DMAs effectively</h3><pre><code class="language-cpp">void restart_output()
{
	static int i_w=0, i_w_flipped=0, i_blocks=0, i_bpa=0, i_arr=0, i_cout=0, i_itr=0, i_layers=i_layers_start;
	static volatile s8 * write_p = layers[i_layers].get_output_pixels_base_p();
	static bool is_new_layer=true;

    static volatile s8 * write_p_old = 0;
	Xil_DCacheFlushRange((UINTPTR)write_p_old, UNITS_EDGES);

	if ((i_itr == 0 &amp;&amp; i_blocks == 31) || (i_itr == 1 &amp;&amp; i_blocks == 0)){
		for (int i=0; i&lt;UNITS_EDGES; i++){
			PRINT(&quot; %d,&quot;, write_p_old[i]);
		}
		PRINT(&quot;] \r\n&quot;);
		PRINT(&quot;(%d,%d,%d,%d-%d,:) -- %p [&quot;, i_arr, i_bpa, i_w_flipped,i_itr,i_cout, write_p);
	}
	write_p_old = write_p;

	// start transfer
	dma_weights_im_out.s2mm_start(	(UINTPTR)write_p,
									layers[i_layers].WORDS_OUT_PER_TRANSFER);

	pad_prev(i_w_flipped,i_blocks,i_bpa,i_arr,i_cout,i_layers);

	// set config
	if (is_new_layer &amp;&amp; i_layers != N_LAYERS-1)
	{
		layers[i_layers].NEXT_P-&gt;set_config();
		layers[i_layers].NEXT_P-&gt;done_write = false;
		is_new_layer = false;
	}

	// PREPARE NEXT INDICES
	// blocks = 31 (a=1,bpa=15), w_f = 191 (w = 190), itr = 0
	if (i_w &lt; layers[i_layers].OUT_W_IN-1)
	{
		i_w += 1;
		// Flip last KW-1 columns : flipped = 2w-(kw+iw)
		// For max: kw &lt;- kw-2
		if (i_w &gt; layers[i_layers].OUT_W_IN - layers[i_layers].KW_PAD)
			i_w_flipped = 2 * layers[i_layers].OUT_W_IN - (i_w + layers[i_layers].KW_PAD);
		else
			i_w_flipped = i_w;
	}
	else
	{
		i_w = 0;
		i_w_flipped = 0;

		 PRINT(&quot; i_blocks: %d, write_p: %p \r\n&quot;, i_blocks, write_p);

		if (i_blocks &lt; layers[i_layers].OUT_BLOCKS-1)
		{
			i_blocks  += 1;
			i_arr      = i_blocks % layers[i_layers].OUT_MAX_FACTOR;
			i_bpa      = i_blocks / layers[i_layers].OUT_MAX_FACTOR;
		}
		else
		{
			i_blocks   = 0;
			i_arr      = 0;
			i_bpa      = 0;

			 PRINT(&quot; i_itr: %d \r\n&quot;, i_itr);

			if (i_itr &gt;= layers[i_layers].ITR-1)
			{
				is_new_layer = true;
				i_itr = 0;
				i_cout= 0;

				if (i_layers &lt; N_LAYERS-1)
					i_layers += 1;
				else
				{
					i_layers = 0;
					done = true;
					PRINT(&quot;All Layers done \r\n&quot;);
				}

				/* Chaining*/
				if (i_layers == N_LAYERS-1)
				{
					layers[0].input_chunk_p = &amp;temp_in_chunk;
					layers[i_layers].output_chunk_p = &amp;temp_out_chunk;
				}
				else
				{
					layers[i_layers].output_chunk_p = get_chunk();
					layers[i_layers].NEXT_P-&gt;input_chunk_p = layers[i_layers].output_chunk_p;
				}
				PRINT(&quot;Writing to new layer: chained_chunks (idx:%d -&gt; idx:%d), data_p= %p \r\n&quot;,
						    layers[i_layers].idx, layers[i_layers].NEXT_P-&gt;idx,
							layers[i_layers].output_chunk_p-&gt;data_p);

				layers[i_layers].print_output_params();
			}
			else if (i_itr == 0)
			{
				i_itr += 1;
				i_cout = layers[i_layers].COUT_VALID;
			}
			else
			{
				i_itr  += 1;
				i_cout += layers[i_layers].EFF_CORES;
			}
		}
	}
	// blocks = 31 (a=1,bpa=15), w_f = 191, itr = 0
	write_p = unravel_image_abwcu(layers[i_layers].get_output_pixels_base_p(),
								  i_arr,i_bpa,i_w_flipped,i_cout,0, i_layers);
}</code></pre><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">14. Hardware (FPGA) Verification</h2></div><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2022/01/image-7.png" class="kg-image" alt="DNN &#x2192; Chip Design [4/4: System-on-Chip]" loading="lazy" width="922" height="674" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-7.png 600w, https://aba-blog.xyz/content/images/2022/01/image-7.png 922w" sizes="(min-width: 720px) 720px"><figcaption>Hardware testing.&#xA0;</figcaption></figure><p></p><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">15. Repeat 11-14</h2></div><p>I spent weeks or months repeating 11-14, to finally get the hardware outputs to match the golden model, and hence the original DNNs. &#xA0;Once I spent a month figuring out a bug where the system worked perfectly in randomized simulations but had wrong values for just 6 bytes out of 4 million bytes. Finally, I found it&apos;s a bug in Vivado&apos;s compiler.</p>]]></content:encoded></item><item><title><![CDATA[DNN → Chip Design [3/4: Digital Design]]]></title><description><![CDATA[<blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><p>After building golden models and understanding the operations, its time to design and implement digital circuits that can accelerate those</p>]]></description><link>https://aba-blog.xyz/dnn-to-chip-3/</link><guid isPermaLink="false">61f5176733068f34ce882ed8</guid><category><![CDATA[Technical Projects]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sat, 29 Jan 2022 10:36:04 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2022/01/vlcsnap-2022-01-30-00h01m19s402.png" medium="image"/><content:encoded><![CDATA[<blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><img src="https://aba-blog.xyz/content/images/2022/01/vlcsnap-2022-01-30-00h01m19s402.png" alt="DNN &#x2192; Chip Design [3/4: Digital Design]"><p>After building golden models and understanding the operations, its time to design and implement digital circuits that can accelerate those operations with: </p><ul><li>low on-chip area</li><li>high fmax (hence short critical paths)</li><li>minimal multiplexers, registers, and SRAM usage</li></ul><p>For this, I first design my modules in detail, on a whiteboard. I spend days or weeks doing this: optimizing designs and mapping out state machines for each module. Once I&apos;m satisfied. I sit with VSCode and start writing synthesizable RTL. Once I&apos;m done, I generate test vectors from the golden model, write testbenches to read and compare them and start debugging.</p><h2 id="steps">Steps:</h2><!--kg-card-begin: markdown--><ol start="3">
<li><strong>Whiteboard</strong>: Design hardware</li>
<li><strong>RTL Design</strong>: SystemVerilog/Verilog for the whiteboard designs</li>
<li><strong>Generate Test Vectors</strong>: using Python Notebooks</li>
<li><strong>Testbenches</strong>: SystemVerilog OOP testbenches to read the input vector (txt file), randomly control the valid &amp; ready signals and get output vectors (txt files)</li>
<li><strong>Debug</strong>: Python notebooks to compare the expected output with simulation output and to find which dimensions have errors.</li>
<li><strong>Microsoft Excel</strong>: I manually simulate the values in wires with excel to debug</li>
<li>Repeat 3-8: For every module &amp; every level of integration</li>
<li><strong>ASIC Synthesis</strong></li>
</ol>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">3. Whiteboard</h2></div><p>I almost always design my modules fully on a whiteboard before sitting down to write RTL. This helps to map out almost every register and multiplexer, get an idea of the critical paths, and also to reduce bugs.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/Data-Flow-of-Inference-Engine-2.png" width="2000" height="1042" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/Data-Flow-of-Inference-Engine-2.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/Data-Flow-of-Inference-Engine-2.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/Data-Flow-of-Inference-Engine-2.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/Data-Flow-of-Inference-Engine-2.png 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/pixel-rot-with-bram.png" width="2000" height="2312" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/pixel-rot-with-bram.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/pixel-rot-with-bram.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/pixel-rot-with-bram.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/pixel-rot-with-bram.png 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/New-Doc-2019-09-12-20.33.10.jpg" width="2000" height="1295" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/New-Doc-2019-09-12-20.33.10.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/New-Doc-2019-09-12-20.33.10.jpg 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/New-Doc-2019-09-12-20.33.10.jpg 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/New-Doc-2019-09-12-20.33.10.jpg 2400w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/IMG_20191209_091223.jpg" width="2000" height="1500" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/IMG_20191209_091223.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/IMG_20191209_091223.jpg 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/IMG_20191209_091223.jpg 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/IMG_20191209_091223.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/Adobe-Scan-Jun-16--2021.png" width="2000" height="1107" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/Adobe-Scan-Jun-16--2021.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/Adobe-Scan-Jun-16--2021.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/Adobe-Scan-Jun-16--2021.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/Adobe-Scan-Jun-16--2021.png 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/LRELU-engine.png" width="2000" height="1350" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/LRELU-engine.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/LRELU-engine.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/LRELU-engine.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/LRELU-engine.png 2400w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/Pad-filter-1.png" width="2000" height="1353" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/Pad-filter-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/Pad-filter-1.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/Pad-filter-1.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/Pad-filter-1.png 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/Quantization-scheme-1.png" width="2000" height="1968" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/Quantization-scheme-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/Quantization-scheme-1.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/Quantization-scheme-1.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/Quantization-scheme-1.png 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/Weights-Rotator.png" width="2000" height="2031" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/Weights-Rotator.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/Weights-Rotator.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/Weights-Rotator.png 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/Weights-Rotator.png 2400w" sizes="(min-width: 720px) 720px"></div></div></div></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">4. RTL Design</h2><h3 class="kg-header-card-subheader">SystemVerilog / Verilog</h3></div><p>I then start writing modules, state-machines...etc. using synthesizable SystemVerilog, converting my whiteboard drawings into code. This is fairly straightforward. I&apos;ve given a stripped-down example code of my conv engine. The key things to note are:</p><ul><li><strong>SystemVerilog </strong>- Verilog lacks a lot of features and has the potential to cause serious bugs. SystemVerilog is beautiful and a breeze to write and read.</li><li><strong>Multidimensional wires and ports</strong> - I use them a lot, to group ports meaningfully and connect with each other. I use the packed dimension, so it can be seamlessly connected to Verilog wrappers which do not accept multiple dimensions.</li><li> <strong>Readability </strong>- I take this seriously. Order does not matter in HDL, but I write in a way that the code top to bottom corresponds to left to right in my whiteboard diagram (the way signal flows).</li><li><strong>Macro Parameters</strong> - I put all the parameters, derived parameters in a common file and include it in all modules. That file itself is written through a tcl file. This way, the parameters of all files are guaranteed to be the same, avoiding bugs and also making the code readable.</li><li><strong>No always@clk</strong>: This might be a surprise. In my entire synthesizable codebase, I have only one sequential always block: in a parametrized module named <em>register.v</em>, which has optional clken, different types of reset...etc. All other modules instantiate this whenever needed. This helps me to avoid bugs, and to visualize the signal flow, as it directly translates from my whiteboard to code.</li><li><strong>FPGA &amp; ASIC</strong> - Using preprocessor directives (`ifdef), I write code to suit both FPGA and ASIC. Registers have async reset in ASIC mode and sync reset in FPGA mode.</li></ul><pre><code class="language-verilog">`timescale 1ns/1ps
`include &quot;../include/params.v&quot;

module conv_engine #(ZERO=0) (
    clk            ,
    clken          ,
    resetn         ,
    s_valid        ,
    s_ready        ,
    s_last         ,
    s_user         ,
    s_data_pixels  ,
    s_data_weights ,
    m_valid        ,
    m_data         ,
    m_last         ,
    m_user         
  );
  input  logic clk, clken, resetn;
  input  logic s_valid, s_last;
  output logic s_ready;
  output logic m_valid, m_last;
  input  logic [`TUSER_WIDTH_CONV_IN-1:0] s_user;
  input  logic [`COPIES-1:0][`UNITS -1:0]                          [`WORD_WIDTH_IN    -1:0] s_data_pixels;
  input  logic [`COPIES-1:0][`GROUPS-1:0][`MEMBERS-1:0]            [`WORD_WIDTH_IN    -1:0] s_data_weights;                                                                        
  output logic [`COPIES-1:0][`GROUPS-1:0][`MEMBERS-1:0][`UNITS-1:0][`WORD_WIDTH_OUT   -1:0] m_data;
  output logic [`TUSER_WIDTH_CONV_OUT-1:0] m_user;
  
  // Code ommited
  logic [`KW_MAX/2:0][`SW_MAX -1:0][`MEMBERS -1:0] lut_sum_start;
  logic [`COPIES-1:0][`GROUPS-1:0][`MEMBERS-1:0][`UNITS-1:0][`WORD_WIDTH_IN*2-1:0] mul_m_data ;
  logic [`COPIES-1:0][`GROUPS-1:0][`MEMBERS-1:0][`UNITS-1:0][`WORD_WIDTH_OUT -1:0] acc_s_data ;
  logic [`COPIES-1:0][`GROUPS-1:0][`MEMBERS-1:0][`UNITS-1:0][`WORD_WIDTH_OUT -1:0] mux_s2_data;

  generate
    genvar c,g,u,m,b,kw2,sw_1;
    // Code ommitted
    for (c=0; c &lt; `COPIES; c++)
      for (g=0; g &lt; `GROUPS; g++)
        for (u=0; u &lt; `UNITS; u++)
          for (m=0; m &lt; `MEMBERS; m++)
            if (m==0) assign mux_s2_data [c][g][m][u] = 0;
            else      assign mux_s2_data [c][g][m][u] = m_data     [c][g][m-1][u];
    assign mux_sel_next = mul_m_valid &amp;&amp; mul_m_user[`I_IS_CIN_LAST] &amp;&amp; (mul_m_kw2 != 0);

    register #(
      .WORD_WIDTH     (1),
      .RESET_VALUE    (0)
    ) MUX_SEL (
      .clock          (clk   ),
      .resetn         (resetn),
      .clock_enable   (clken ),
      .data_in        (mux_sel_next),
      .data_out       (mux_sel )
    );
    assign clken_mul = clken &amp;&amp; !mux_sel;
            
    for (m=0; m &lt; `MEMBERS; m++) begin: Mb
      for (kw2=0; kw2 &lt;= `KW_MAX/2; kw2++)
        for (sw_1=0; sw_1 &lt; `SW_MAX; sw_1++) begin
          localparam k = kw2*2 + 1;
          localparam s = sw_1 + 1;
          localparam j = k + s -1;

          assign lut_sum_start[kw2][sw_1][m] = m % j &lt; s; // m % 3 &lt; 1 : 0,1
        end
      assign acc_m_sum_start [m] = lut_sum_start[acc_m_kw2][acc_m_sw_1][m] &amp; acc_m_user[`I_IS_SUM_START];
      // Code ommited
    end

    for (c=0; c &lt; `COPIES; c++) begin: Ca
      for (g=0; g &lt; `GROUPS; g++) begin: Ga
        for (u=0; u &lt; `UNITS; u++) begin: Ua
          for (m=0; m &lt; `MEMBERS; m++) begin: Ma
            processing_element PROCESSING_ELEMENT (
              .clk           (clk           ),
              .clken         (clken         ),
              .resetn        (resetn        ),
              .clken_mul     (clken_mul     ),
              .s_data_pixels (s_data_pixels [c]      [u]), 
              .s_data_weights(s_data_weights[c][g][m]   ),
              .mul_m_data    (mul_m_data    [c][g][m][u]),
              .mux_sel       (mux_sel       ),
              .mux_s2_data   (mux_s2_data   [c][g][m][u]),
              .bypass        (bypass        [m]),
              .clken_acc     (clken_acc     [m]),
              .acc_s_data    (acc_s_data    [c][g][m][u]),
              .m_data        (m_data        [c][g][m][u])
            );
    end end end end

    // Code ommitted
    assign m_user_base[`I_IS_BOTTOM_BLOCK:`I_IS_NOT_MAX] = acc_m_user[`I_IS_BOTTOM_BLOCK:`I_IS_NOT_MAX];
    assign m_user  = {m_clr, m_shift_b, m_shift_a, m_user_base};
  endgenerate
endmodule</code></pre><pre><code class="language-verilog">module processing_element (
    clk    ,
    clken  ,
    resetn ,

    clken_mul,
    s_data_pixels, 
    s_data_weights,
    mul_m_data,

    mux_sel,
    mux_s2_data,
    bypass,
    clken_acc,
    acc_s_data,
    m_data
  );
  input  logic clk, clken, resetn;
  input  logic clken_mul, mux_sel, bypass, clken_acc;
  input  logic [`WORD_WIDTH_IN  -1:0] s_data_pixels, s_data_weights;
  input  logic [`WORD_WIDTH_OUT -1:0] mux_s2_data;
  output logic [`WORD_WIDTH_IN*2-1:0] mul_m_data;
  output logic [`WORD_WIDTH_OUT -1:0] acc_s_data;
  output logic [`WORD_WIDTH_OUT -1:0] m_data;

`ifdef MAC_XILINX
  multiplier MUL (
`else
  multiplier_raw MUL (
`endif
    .CLK    (clk),
    .CE     (clken_mul),
    .A      (s_data_pixels ),
    .B      (s_data_weights),
    .P      (mul_m_data    )
  );
  assign acc_s_data = mux_sel ? mux_s2_data  : `WORD_WIDTH_OUT&apos;(signed&apos;(mul_m_data));
  
`ifdef MAC_XILINX
  accumulator ACC (
`else
  accumulator_raw ACC (
`endif
    .CLK    (clk),  
    .bypass (bypass     ),  
    .CE     (clken_acc  ),  
    .B      (acc_s_data ),  
    .Q      (m_data     )  
  );
endmodule</code></pre><p></p><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">5. Test Vector Generation</h2><h3 class="kg-header-card-subheader">Python</h3></div><p>Next, I write python functions to extracts weights and inputs of each layer from my custom framework model (2.2), and convert them into input test vectors. Their dimensions need to be split, reshaped, transposed and flattened to get the final input \(\hat{X}\), and weights \(\hat{K}\) packets which can be understood by the hardware I designed. Output \(\hat{Y}\) is also transformed to match the hardware&apos;s outputs. Also, configuration bits need to be calculated and appended to the packet to make it complete.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/reshape.png" width="1088" height="1305" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/reshape.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/reshape.png 1000w, https://aba-blog.xyz/content/images/2022/01/reshape.png 1088w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/reshape2.png" width="1074" height="1590" loading="lazy" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/reshape2.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/reshape2.png 1000w, https://aba-blog.xyz/content/images/2022/01/reshape2.png 1074w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>The python functions perform these reshapes to generate input and output vectors (multidimensional tensors)</figcaption></figure><pre><code class="language-python">def get_weights(i_layers, i_itr, c):
    weights = c.LAYERS[f&apos;{c.PREFIX_CONV}{i_layers}&apos;].weights
    KH, KW, CIN, COUT = weights.shape
    max_factor = 2 if f&apos;{c.PREFIX_MAX}{i_layers}&apos; in c.LAYERS.keys() else 1
    print(f&quot;get_weights - shape_in:(KH, KW, CIN, COUT) = {weights.shape}&quot;)
    
    &apos;&apos;&apos;
    Reshape
    &apos;&apos;&apos;
    weights = weights.transpose(3,0,1,2) #(COUT,KH,KW,CIN)
    weights = fill_invalid_scg(weights,KW=KW,max_factor=max_factor,c=c) #(ITR,EFF_CORES,KH,KW,CIN)
    ITR,EFF_CORES = weights.shape[0:2]
    weights = weights.transpose(0,4,2,1,3) #(ITR,CIN,KH,EFF_CORES,KW)

    &apos;&apos;&apos;
    * Data comes out of maxpool in the order: S,CGU
    * Data comes out of conv in the order   : CGMU and is transposed into S,CGUby hardware
    * Conv in takes weights in order        : CGM

    * Since system_out is SCG, first invalid should be filled that way, so that output data is continous and cin matches cout
    * After filling, we transpose it to CGM
    &apos;&apos;&apos;
    SUB_CORES = c.MEMBERS//KW
    weights = weights.reshape((ITR,CIN,KH, SUB_CORES,c.COPIES//max_factor,c.GROUPS ,KW)) # EFF_CORES = (SCG)
    weights = weights.transpose(0,1,2, 4,5, 3,6) # CGS
    weights = weights.reshape((ITR,CIN,KH,1,c.COPIES//max_factor,c.GROUPS,SUB_CORES,KW)) # (CGS)
    weights = np.repeat(weights,repeats=max_factor,axis=3)
    weights = weights.reshape((ITR,CIN,KH,c.COPIES,c.GROUPS,SUB_CORES,KW))
    weights = weights.reshape((ITR,CIN,KH,c.COPIES,c.GROUPS,SUB_CORES*KW))
    zeros = np.zeros((ITR,CIN,KH,c.COPIES,c.GROUPS,c.MEMBERS), dtype=weights.dtype)
    zeros[:,:,:,:,:,0:SUB_CORES*KW] = weights
    weights = zeros

    KERNEL_BEATS = CIN*KH
    weights = weights.reshape(ITR,KERNEL_BEATS,c.COPIES,c.GROUPS,c.MEMBERS)

    &apos;&apos;&apos;
    Add LRELU Beats
    &apos;&apos;&apos;
    lrelu = get_lrelu_config(i_layers=i_layers,c=c) 
        
    LRELU_BEATS = lrelu.shape[1]
    weights_beats = np.concatenate([lrelu,weights], axis=1) # (ITR, LRELU_BEATS + KERNEL_BEATS, COPIES, GROUPS, MEMBERS)

    _,H,W,CIN = c.LAYERS[f&apos;{c.PREFIX_CONV}{i_layers}&apos;].in_data.shape
    BLOCKS    = H // (SH*max_factor*c.CONV_UNITS)

    bram_weights_addr_max = LRELU_BEATS + SW*KH*CIN-1
    print(&quot;bram_weights_addr_max: &quot;, bram_weights_addr_max)

    weights_config = 0
    weights_config |= (KW//2)
    weights_config |= (KH//2)               &lt;&lt; (BITS_KW2)
    weights_config |= SW-1                  &lt;&lt; (BITS_KW2 + BITS_KH2)
    weights_config |= (CIN   -1)            &lt;&lt; (BITS_KW2 + BITS_KH2 + BITS_SW)
    weights_config |= (W     -1)            &lt;&lt; (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX)
    weights_config |= (BLOCKS-1)            &lt;&lt; (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX + BITS_COLS_MAX)
    weights_config |= bram_weights_addr_max &lt;&lt; (BITS_KW2 + BITS_KH2 + BITS_SW + BITS_CIN_MAX + BITS_COLS_MAX + BITS_BLOCKS_MAX)

    weights_config = np.frombuffer(np.uint64(weights_config).tobytes(),np.int8)
    weights_config = np.repeat(weights_config[np.newaxis,...],repeats=ITR,axis=0)

    &apos;&apos;&apos;
    ADD c BEATS
    &apos;&apos;&apos;
    weights_dma_beats = np.concatenate([weights_config,weights_beats.reshape(ITR,-1)], axis=1)

    assert weights_dma_beats.shape == (ITR, 8 + (LRELU_BEATS + CIN*KH*SW)*c.COPIES*c.GROUPS*c.MEMBERS)
    print(f&quot;get_weights - weights_dma_beats.shape: (ITR, 4 + (LRELU_BEATS + CIN*KH)*COPIES*GROUPS*MEMBERS) = {weights_dma_beats.shape}&quot;)

    np.savetxt(f&quot;{c.DATA_DIR}{i_layers}_weights.txt&quot;, weights_dma_beats[i_itr].flatten(), fmt=&apos;%d&apos;)
    return weights_dma_beats</code></pre><p></p><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">6. Testbench &amp; Simulation</h2><h3 class="kg-header-card-subheader">SystemVerilog</h3></div><p>Next, I write testbenches for the modules. They are built around two custom SystemVerilog classes: AXIS_Slave, which reads a text file and loads data into an AXI stream port while conforming to the protocol, and AXIS_Master which reads data from a port and writes into a text file. </p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/jZIaMTLCzjc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>The control signals: valid and ready are randomized. They can be switched on and off according to a given probability, to simulate the effects of memory bus freezing up and clearing.</p><figure class="kg-card kg-code-card"><pre><code class="language-verilog">`timescale 1ns/1ps
class AXIS_Slave #(WORD_WIDTH=8, WORDS_PER_BEAT=1, VALID_PROB=100);

  string file_path;
  int words_per_packet, status, iterations, i_words;
  int file = 0;
  int i_itr = 0;
  bit enable = 0;
  bit first_beat = 1;

  rand bit s_valid;
  constraint c { s_valid dist { 0 := (100-VALID_PROB), 1 := (VALID_PROB)}; };

  function new(string file_path, int words_per_packet, int iterations);
    this.file_path = file_path;
    this.words_per_packet = words_per_packet;
    this.iterations = iterations;
    this.file = $fopen(this.file_path, &quot;r&quot;);
  endfunction

  function void fill_beat(
    ref logic [WORDS_PER_BEAT-1:0][WORD_WIDTH-1:0] s_data, 
    ref logic [WORDS_PER_BEAT-1:0] s_keep,
    ref logic s_last);

    if($feof(file)) $fatal(1, &quot;EOF found\n&quot;);
    if (first_beat) first_beat = 0;

    for (int i=0; i &lt; WORDS_PER_BEAT; i++) begin
      status = $fscanf(file,&quot;%d\n&quot;, s_data[i]);
      s_keep[i] = i_words &lt; words_per_packet;
      i_words  += 1;
    end
    if(i_words &gt;= words_per_packet)
      s_last = 1;
  endfunction

  function void reset(
    ref logic s_valid,
    ref logic [WORDS_PER_BEAT-1:0][WORD_WIDTH-1:0] s_data, 
    ref logic [WORDS_PER_BEAT-1:0] s_keep,
    ref logic s_last
  );
    enable = 0;
    s_data = &apos;{default:0};
    s_valid = 0;
    s_keep = 0;
    s_last = 0;
    first_beat = 1;
    i_words = 0;

    if (file != 0) $fclose(file);
    file = $fopen(file_path, &quot;r&quot;);
  endfunction

  task axis_feed(
    ref logic aclk,
    ref logic s_ready,
    ref logic s_valid,
    ref logic [WORDS_PER_BEAT-1:0][WORD_WIDTH-1:0] s_data, 
    ref logic [WORDS_PER_BEAT-1:0] s_keep,
    ref logic s_last
  );
    // Before beginning: set all signals zero
    if (~enable) begin
      this.reset(s_valid, s_data, s_keep, s_last);
      @(posedge aclk);
      return;
    end

    @(posedge aclk);
    this.randomize(); // random this.s_valid at every cycle
    if (s_ready &amp;&amp; (first_beat ? this.s_valid : s_valid)) begin
      // If s_last has passed with handshake, packet done. start next itr
      if(s_last) begin #1;
        i_itr += 1;
        this.reset(s_valid, s_data, s_keep, s_last);

        if (i_itr &lt; iterations) 
          enable = 1;
        else begin
          $fclose(file);
          return;
        end
      end
      else #1;

      // If file is not open, keep valid down.
      if(file == 0) begin
        s_valid = 0;
        return;
      end
      else this.fill_beat(s_data, s_keep, s_last);
    end
    else #1;

    if (~first_beat) s_valid = this.s_valid;
  endtask
endclass</code></pre><figcaption>Stripped down version of AXIS_Slave class</figcaption></figure><p>Following is an example on how AXIS slave and master classes are utilized. Each module gets a testbench like this. Some modules get multiple slave and multiple masters.</p><pre><code class="language-verilog">module axis_tb_demo();
  timeunit 1ns;
  timeprecision 1ps;
  localparam CLK_PERIOD = 10;
  logic aclk;
  initial begin
    aclk = 0;
    forever #(CLK_PERIOD/2) aclk &lt;= ~aclk;
  end

  localparam WORD_WIDTH        = 8;
  localparam WORDS_PER_PACKET  = 40;
  localparam WORDS_PER_BEAT    = 4;
  localparam ITERATIONS        = 6;
  localparam BEATS = int&apos;($ceil(real&apos;(WORDS_PER_PACKET)/real&apos;(WORDS_PER_BEAT)));

  logic [WORDS_PER_BEAT  -1:0][WORD_WIDTH-1:0] data;
  logic [WORDS_PER_BEAT  -1:0] keep;
  logic valid, ready, last;

  string path = &quot;D:/cnn-fpga/data/axis_test.txt&quot;;
  string out_base = &quot;D:/cnn-fpga/data/axis_test_out_&quot;;

  AXIS_Slave #(
    .WORD_WIDTH    (WORD_WIDTH    ), 
    .WORDS_PER_BEAT(WORDS_PER_BEAT), 
    .VALID_PROB    (70            )
    ) slave_obj  = new(
      .file_path       (path), 
      .words_per_packet(WORDS_PER_PACKET), 
      .iterations      (ITERATIONS)
      );
  AXIS_Master #(
    .WORD_WIDTH    (WORD_WIDTH    ), 
    .WORDS_PER_BEAT(WORDS_PER_BEAT), 
    .READY_PROB    (70            ), 
    .CLK_PERIOD    (CLK_PERIOD    ),
    .IS_ACTIVE     (1             )
    ) master_obj = new(
      .file_base(out_base),
      .words_per_packet(-1),
      .packets_per_file(2)
      );
  initial forever  slave_obj.axis_feed(aclk, ready, valid, data, keep, last);
  initial forever master_obj.axis_read(aclk, ready, valid, data, keep, last);

  initial begin
    @(posedge aclk);
    slave_obj.enable &lt;= 1;
    master_obj.enable &lt;= 1;
  end

  int s_words, s_itr, m_words, m_itr, m_packets, m_packets_per_file;
  initial
    forever begin
      @(posedge aclk);
      s_words = slave_obj.i_words;
      s_itr = slave_obj.i_itr;
      m_words = master_obj.i_words;
      m_itr = master_obj.i_itr;
      m_packets = master_obj.i_packets;
      m_packets_per_file = master_obj.packets_per_file;
    end
endmodule</code></pre><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">7. Debugging</h2><h3 class="kg-header-card-subheader">Python Notebooks &amp; SystemVerilog Simulations</h3></div><p>I then run simulations, collect output vectors and compare them with expected vectors using python notebooks. Notebooks allow one to play around with data, quickly print and observe different dimensions...etc. </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2022/01/image-6.png" class="kg-image" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" loading="lazy" width="1134" height="759" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-6.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-6.png 1000w, https://aba-blog.xyz/content/images/2022/01/image-6.png 1134w" sizes="(min-width: 720px) 720px"><figcaption>Integration testing the whole system</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2022/01/image-8.png" class="kg-image" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" loading="lazy" width="816" height="381" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-8.png 600w, https://aba-blog.xyz/content/images/2022/01/image-8.png 816w" sizes="(min-width: 720px) 720px"><figcaption>Observing outputs</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2022/01/image-9.png" class="kg-image" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" loading="lazy" width="464" height="276"><figcaption>The dimension indices of the locations where output doesn&apos;t match expected output. Very helpful in debugging.</figcaption></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">8. Debugging with Microsoft Excel</h2><h3 class="kg-header-card-subheader">Yep :-)</h3></div><p>In some cases, the output from a module is garbage and does not match the expected output at all. Since it is a convolution over several values, it is near impossible to guess the bug by looking at such garbage numbers. In that case, I resort to Excel, where I manually transform a set of small input vectors through the logic, step by step to see what I should expect in every clock cycle. I then compare it to the waveforms I see in the simulator to figure out where the bug is.</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2022/01/image-13.png" class="kg-image" alt="DNN &#x2192; Chip Design [3/4: Digital Design]" loading="lazy" width="1920" height="932" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-13.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-13.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/image-13.png 1600w, https://aba-blog.xyz/content/images/2022/01/image-13.png 1920w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">9. Repeat</h2></div><p>I move back and forth between the whiteboard, RTL code, python code, and simulation to fix bugs one by one. Some take weeks and make me want to pull my hair out. I also do this for each module, then put them together hierarchically, write integration testbenches, and test that too. </p><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">10. ASIC Synthesis</h2></div><p>Once the design is verified in randomized simulations, I write the scripts for ASIC synthesis. Our university uses Cadence tools, so the following script is for Cadence Genus, using 65nm CMOS PDK from TSMC.</p><pre><code class="language-tcl">set TOP axis_accelerator_asic
# set TOP axis_conv_engine

#--------- CONFIG
set RTL_DIR ../../rtl
set XILINX 0
source ../../tcl/config.tcl
set_db hdl_max_loop_limit 10000000

set TECH 65nm
set NUM_MACS [expr $MEMBERS*$UNITS*$GROUPS*$COPIES]
set REPORT_DIR ../report/${TECH}/${TOP}/${NUM_MACS}
exec mkdir -p $REPORT_DIR

#--------- LIBRARIES
set LIB_DIR ../../../tsmc/${TECH}/GP
set_db library [glob $LIB_DIR/cc_lib/noise_scadv10_cln65gp_hvt_tt_1p0v_25c.lib $LIB_DIR/cc_lib/scadv10_cln65gp_hvt_tt_1p0v_25c.lib]
set_db lef_library [glob $LIB_DIR/lef/tsmc_cln65_a10_6X1Z_tech.lef $LIB_DIR/lef/tsmc_cln65_a10_6X2Z_tech.lef $LIB_DIR/lef/tsmc65_hvt_sc_adv10_macro.lef]
set_db qrc_tech_file $LIB_DIR/other/icecaps.tch
# set LIB_DIR ../../../tsmc/${TECH}/LP
# set_db library [glob $LIB_DIR/lib/sc12_cln65lp_base_hvt_tt_typical_max_1p00v_25c.lib $LIB_DIR/lib/sc12_cln65lp_base_hvt_tt_typical_max_1p20v_25c.lib]
# set_db lef_library [glob $LIB_DIR/lef/sc12_cln65lp_base_hvt.lef]

#--------- READ
read_hdl -mixvlog [glob $RTL_DIR/include/*]
read_hdl -mixvlog [glob $RTL_DIR/external/*]
read_hdl -mixvlog [glob $RTL_DIR/src/*]

#--------- ELABORATE &amp; CHECK
set_db lp_insert_clock_gating true
elaborate $TOP
check_design &gt; ${REPORT_DIR}/check_design.log
uniquify $TOP

#--------- CONSTRAINTS
set PERIOD [expr 1000.0/$FREQ_HIGH]
create_clock -name aclk -period $PERIOD [get_ports aclk]
set_dont_touch_network [all_clocks]
set_dont_touch_network [get_ports {aresetn}]

set design_inputs [get_ports {m_axis_tready s_axis_pixels_tvalid s_axis_pixels_tlast s_axis_pixels_tdata s_axis_pixels_tkeep s_axis_weights_tvalid s_axis_weights_tlast s_axis_weights_tdata s_axis_weights_tkeep}]
set design_outputs [get_ports {s_axis_pixels_tready  s_axis_weights_tready m_axis_tvalid m_axis_tlast m_axis_tdata m_axis_tkeep}]

set_input_delay  [expr $PERIOD * 0.6] -clock aclk $design_inputs
set_output_delay [expr $PERIOD * 0.6] -clock aclk $design_outputs

#--------- RETIME OPTIONS
set_db retime_async_reset true
set_db design:${TOP} .retime true

#--------- SYNTHESIZE
set_db syn_global_effort high
syn_generic
syn_map
syn_opt

#--------- NETLIST
write -mapped &gt; ../output/${TOP}.v
write_sdc &gt; ../output/${TOP}.sdc

#--------- REPORTS
report_area &gt; ${REPORT_DIR}/area.log
report_gates &gt; ${REPORT_DIR}/gates.log
report_timing  -nworst 10 &gt; ${REPORT_DIR}/timing.log
report_congestion &gt; ${REPORT_DIR}/congestion.log
report_messages &gt; ${REPORT_DIR}/messages.log
report_hierarchy &gt; ${REPORT_DIR}/hierarchy.log
report_clock_gating &gt; ${REPORT_DIR}/clock_gating.log

build_rtl_power_models -clean_up_netlist
report_power &gt; ${REPORT_DIR}/power.log</code></pre><h2 id="next">Next:</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-4/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [4/4: System-on-Chip]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper SoC Block Design: Build FPGA projects with Vivado manually and synthesize Automation: TCL scripts to automate the project building and configur&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [3/4: Digital Design]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/image_o-12-1.png" alt="DNN &#x2192; Chip Design [3/4: Digital Design]"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[DNN → Chip Design [2/4: Golden Models]]]></title><description><![CDATA[<p></p><blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><p>Golden Models are essential to hardware (FPGA/ASIC) development. They model the expected behavior of a chip using a high-level</p>]]></description><link>https://aba-blog.xyz/dnn-to-chip-2/</link><guid isPermaLink="false">61f5163733068f34ce882ebb</guid><category><![CDATA[Technical Projects]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sat, 29 Jan 2022 10:30:50 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" alt="DNN &#x2192; Chip Design [2/4: Golden Models]"><p></p><blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><p>Golden Models are essential to hardware (FPGA/ASIC) development. They model the expected behavior of a chip using a high-level language, such that they can be built relatively fast, with almost zero chance of error. The input and expected output test vectors for every RTL module are generated using them, and the simulation output from the testbench is compared against their &apos;gold standard.&apos;</p><p>I first obtain pretrained DNNs from PyTorch / Tensorflow model zoo, analyze them, then load them into the custom DNN inference framework I have built with NumPy stack to ensure I fully understand each operation. I then generate test vectors from those golden models.</p><h2 id="steps">Steps:</h2><!--kg-card-begin: markdown--><ol>
<li><strong>PyTorch/TensorFlow</strong>: Explore DNN models, quantize &amp; extract weights</li>
<li><strong>Golden Model in Python (NumPy stack)</strong>: Custom OOP framework, process the weights, convert to custom datatypes</li>
</ol>
<!--kg-card-end: markdown--><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">1. Tensorflow / PyTorch</h2></div><p>Tensorflow (Google) and PyTorch (Facebook) are the two competing open source libraries used to build, train, quantize and deploy modern deep neural networks. </p><p>Both frameworks provide high-level, user-friendly classes and functions such as Conv2D, model.fit() to build &amp; train networks. Each such high-level API is implemented using their own low-level tensor operations (matmul, einsum), which also can be used by the users. Those operations are implemented using their C++ backend, accelerated by high performant libraries like eigen and CUDA. Once we define the models using Python, the C++ code underneath pulls the load, making them fast as well as user-friendly.</p><h3 id="11-download-explore-pretrained-dnn-models">1.1. Download &amp; Explore Pretrained DNN Models</h3><p>As the first step, I obtained the pretrained models from either <a href="https://keras.io/api/applications/">Keras.Applications</a> or <a href="https://pytorch.org/serve/model_zoo.html">PyTorch Model Zoo</a>.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import tensorflow as tf
from tensorflow.keras.applications import VGG16

vgg16 = VGG16(
    include_top=True,
    weights=&quot;imagenet&quot;,
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation=&quot;softmax&quot;,
)

vgg16.save(&apos;saved_model/vgg16&apos;)
vgg16.summary()</code></pre><figcaption>Get VGG16 from TensorFlow zoo (Keras.Applications)</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
model = torch.hub.load(&apos;pytorch/vision:v0.10.0&apos;, &apos;alexnet&apos;, pretrained=True)
model.eval()
torch.save(model, &quot;alexnet.pt&quot;)</code></pre><figcaption>Get AlexNet from PyTorch Model Zoo</figcaption></figure><h3 id="12-build-models-retrain-if-needed-pytorch">1.2. <strong>Build Models &amp; </strong>Retrain if needed (PyTorch)</h3><p>PyTorch is more intuitive, pythonic and bliss to work with. I use it to build new models and train them if needed. </p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets
from torchvision import transforms as T
from torch.optim.lr_scheduler import StepLR

H = 28
N = 32
device = torch.device(&quot;cuda&quot;)

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(H**2, 32)
        self.fc2 = nn.Linear(32, 10)
    def forward(self, x):
        x = self.fc1(x)
        x = F.leaky_relu(x,0.01)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output
        
model = Net().to(device)</code></pre><figcaption>Building a simple fully connected network in PyTorch</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python">&apos;&apos;&apos; Create Data Loaders to efficiently pull data &apos;&apos;&apos;
transform = T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))])
dataset1  = datasets.MNIST(&apos;../data&apos;, train=True, download=True, transform=transform)
dataset2  = datasets.MNIST(&apos;../data&apos;, train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(dataset1,batch_size=N)
test_loader = torch.utils.data.DataLoader(dataset2,batch_size=N)

&apos;&apos;&apos; Functions to Test &amp; Train &apos;&apos;&apos;

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for (data, target) in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        print(f&apos;Train, epoch: {epoch}, loss: {loss.item():.6f}&apos;)

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction=&apos;sum&apos;).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    print(f&apos;Test, loss: {test_loss}, acc: {100*correct/len(test_loader.dataset):.0f}&apos;)

optimizer = optim.Adadelta(model.parameters(), lr=1.0)
scheduler = StepLR(optimizer, step_size=1, gamma=0.7)

&apos;&apos;&apos; Train for 50 Epochs &apos;&apos;&apos;
for epoch in range(1, 51):
    train(model, device, train_loader, optimizer, epoch)
    test(model, device, test_loader)
    scheduler.step()

&apos;&apos;&apos; Save Trained Model &apos;&apos;&apos;
torch.save(model, &quot;mnist_cnn.pt&quot;)</code></pre><figcaption>Training the model in PyTorch with MNIST dataset</figcaption></figure><h3 id="13-convert-torch-models-to-tensorflow">1.3. Convert Torch Models to Tensorflow</h3><p>However, the support for int8 quantization for PyTorch is still experimental. Therefore, for most of my work, I use pretrained models from Tensorflow, whose quantization library (TFLite) is much superior. </p><p>Some models, like AlexNet, are not found in Keras.Applications. Therefore, I load them from PyTorch Model Zoo and convert them to ONNX (the common open-source format) and then load them in Tensorflow.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
from PIL import Image
from torchvision import transforms

model = torch.load(&apos;alexnet.pt&apos;)

input_image = Image.open(&apos;dog.jpg&apos;)
preprocess = transforms.Compose([
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0)

torch.onnx.export(model, input_batch, &quot;alexnet.onnx&quot;,
		input_names=[&apos;input&apos;], output_names=[&apos;output&apos;])</code></pre><figcaption>Open the saved AlexNet model and export as ONNX</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python"># First install onnx2keras with: pip install onnx onnx2keras
import onnx
from onnx2keras import onnx_to_keras
onnx_model = onnx.load(&quot;alexnet.onnx&quot;)
k_model = onnx_to_keras(onnx_model, [&apos;input&apos;], change_ordering=True)
k_model.save(&apos;saved_model/alexnet&apos;)</code></pre><figcaption>Convert ONNX model to Keras (Tensorflow)</figcaption></figure><h3 id="14-quantize-models-with-tensorflowlite">1.4. Quantize Models with TensorFlowLite</h3><p>Following is an example of loading a float32 model (VGG16) from tensorflow&apos;s savedmodel format (1.1), testing it, quantizing it to int8, and testing &#xA0;&amp; saving the quantized network.</p><pre><code class="language-python">import tensorflow as tf
filenames = glob(&quot;dataset/*.jpg&quot;)

&apos;&apos;&apos;
LOAD AND TEST FLOAT32 MODEL
&apos;&apos;&apos;
prep_fn = tf.keras.applications.vgg16.preprocess_input
model = tf.keras.models.load_model(f&apos;saved_model/vgg16&apos;)
h = model.input_shape[1]

import cv2
from glob import glob
import numpy as np
def representative_data_gen():
    for im_path in filenames:
        im = cv2.imread(im_path)
        im = cv2.resize(im, (h,h))
        im = im[None,:,:,::-1]
        im = prep_fn(im)
        im = tf.convert_to_tensor(im)
        yield [im]

images = list(representative_data_gen())

predictions = np.zeros((len(images),), dtype=int)
for i, image in enumerate(images):
    output = model(image[0])[0]
    predictions[i] = output.numpy().argmax()
print(predictions)

&apos;&apos;&apos;
CONVERT AND SAVE INT8 MODEL (STATIC QUANTIZATION)
&apos;&apos;&apos;
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model_quant = converter.convert()

import pathlib
tflite_model_quant_file = pathlib.Path(f&quot;tflite/vgg16.tflite&quot;)
tflite_model_quant_file.write_bytes(tflite_model_quant)

&apos;&apos;&apos;
LOAD AND TEST QUANTIZED MODEL
&apos;&apos;&apos;
interpreter = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()[0]
output_details = interpreter.get_output_details()[0]

images = list(representative_data_gen())
predictions = np.zeros((len(images),), dtype=int)

for i, image in enumerate(images):
    image = image[0]
    input_scale, input_zero_point = input_details[&quot;quantization&quot;]
    image = image / input_scale + input_zero_point

    test_image = image.numpy().astype(input_details[&quot;dtype&quot;])
    interpreter.set_tensor(input_details[&quot;index&quot;], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[&quot;index&quot;])[0]
    predictions[i] = output.argmax()

print(predictions)</code></pre><h3 id="15-explore-model-architecture">1.5 Explore Model Architecture</h3><p><a href="https://netron.app/">Netron </a>is a great tool for opening tensorflow&apos;s 32-bit models (savedmodel), tflite&apos;s int8 models (tflite), pytorch models (pt), ONNX models, and more, to observe the architecture and tensor names.</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2022/01/image-2.png" class="kg-image" alt="DNN &#x2192; Chip Design [2/4: Golden Models]" loading="lazy" width="1062" height="1042" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-2.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-2.png 1000w, https://aba-blog.xyz/content/images/2022/01/image-2.png 1062w" sizes="(min-width: 720px) 720px"></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">2. Golden Model</h2><h3 class="kg-header-card-subheader">Python (NumPy stack)</h3></div><p>After obtaining the pretrained model, I need to 100% understand what operations are involved and how they are applied as data flows through the network. The best way to do this is to re-do it myself from scratch and obtain exactly the same results. </p><h3 id="21-custom-quantization-scheme">2.1. Custom Quantization Scheme</h3><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2022/01/quant2.jpg" class="kg-image" alt="DNN &#x2192; Chip Design [2/4: Golden Models]" loading="lazy" width="1280" height="606" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/quant2.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/quant2.jpg 1000w, https://aba-blog.xyz/content/images/2022/01/quant2.jpg 1280w" sizes="(min-width: 720px) 720px"></figure><h3 id="22-custom-inference-framework-oop-python">2.2. Custom Inference Framework (OOP, Python)</h3><p>For this, I built a custom framework in Python. It is structured like Keras with the following classes, inheriting as follows:</p><!--kg-card-begin: markdown--><ul>
<li>MyModel</li>
<li>MyLayer
<ul>
<li>MyConv</li>
<li>MyLeakyReLU</li>
<li>MyMaxpool</li>
<li>MyConcat</li>
<li>MySpaceToDepth</li>
<li>MyFlatten</li>
</ul>
</li>
</ul>
<!--kg-card-end: markdown--><p>A MyModel object has a list of objects from MyLayer&apos;s children&apos;s classes. It&apos;s constructor extracts weights from tflite and sets them to the layers. A set of images can flow through the layers through a recursive call to the last layer. Following is the stripped-down version of the MyConv implementation.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">class MyConv(MyLayer):
    def __init__(self,
                 weights_biases,
                 prev_layer=None,
                 bn_weights=None,
                 name=&apos;&apos;,
                 np_dtype=np.float64,
                 np_dtype_sum=np.float64,
                 np_dtype_conv_out=np.float64,
                 float_dtype=np.float64,
                 bits_conv_out=32,
                 quantize=False,
                 float_ieee=True):

        MyLayer.__init__(self,
                         name=name,
                         prev_layer=prev_layer,
                         np_dtype=np_dtype,
                         quantize=quantize,
                         float_ieee=float_ieee)
        self.np_dtype_sum = np_dtype_sum
        self.np_dtype_conv_out = np_dtype_conv_out
        self.float_dtype = float_dtype

        assert len(weights_biases[0].shape) == 4
        self.weights = weights_biases[0].astype(self.np_dtype)
        self.weights_flipped = np.flip(
            self.weights, [0, 1]).astype(self.np_dtype)
        self.kernel = self.weights.shape[0:2]
        self.in_ch_n = self.weights.shape[2]
        self.out_ch_n = self.weights.shape[3]
        self.biases = weights_biases[1].astype(self.np_dtype)

        self.fuse_bn(bn_weights)

        if self.quantize:
            self.clip_max = 2**(bits_conv_out-1)-1
            self.clip_min = -2**(bits_conv_out-1)

    def np_out(self, in_data):
        if self.quantize:
            in_data = self.in_data.copy().astype(self.np_dtype_sum)
            weights = self.weights.copy().astype(self.np_dtype_sum)

            self.np_out_data = self.conv2d_einsum(in_data, weights)

            self.np_out_data = np.clip(self.np_out_data, self.clip_min, self.clip_max)
            self.np_out_data = self.np_out_data.astype(self.np_dtype_conv_out)
        else:
            out = self.conv2d_einsum(self.in_data, self.weights)
            out += self.biases
            self.np_out_data = self.decode(self.encode(out))
        return self.np_out_data

    def fuse_bn(self, bn_weights, epsilon=0.001):
        self.gamma, self.beta, self.mean, self.variance = bn_weights
        self.epsilon = epsilon

        scale = self.gamma / np.sqrt(self.variance + self.epsilon)

        self.pure_weights = self.weights.copy()
        self.pure_biases = self.biases.copy()

        self.weights = self.weights * scale
        self.weights_flipped = np.flip(self.weights, [0, 1])
        self.biases = beta + scale * (self.biases - self.mean)

    @staticmethod
    def conv2d_einsum(img, kernel):
        pad_h = kernel.shape[0]//2
        pad_w = kernel.shape[1]//2

        out_batch = []
        for n in range(img.shape[0]):
            padding = ((pad_h, pad_h), (pad_w, pad_w), (0, 0))
            img_pad = np.pad(img[n], padding, &apos;constant&apos;)

            sub_shape = tuple(np.subtract(img_pad.shape, kernel.shape[0:-1])+1)
            shape = kernel.shape[0:-1] + sub_shape
            strd = np.lib.stride_tricks.as_strided
            submatrices = strd(img_pad,shape,img_pad.strides*2,writeable=False)

            out_batch += [np.einsum(&apos;ijkl,ijkmno-&gt;mnl&apos;, kernel, submatrices)]
        return np.array(out_batch)</code></pre><figcaption>stripped-down version of the MyConv implementation</figcaption></figure><h3 id="23-rebuilding-the-model-debugging">2.3. Rebuilding the model &amp; Debugging</h3><p>I then rebuild the model using the above framework, pass data and tweak things until I get the exact same output. That tells me I have understood all the operations going on inside the model.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image-3.png" width="723" height="862" loading="lazy" alt="DNN &#x2192; Chip Design [2/4: Golden Models]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-3.png 600w, https://aba-blog.xyz/content/images/2022/01/image-3.png 723w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image-4.png" width="1331" height="655" loading="lazy" alt="DNN &#x2192; Chip Design [2/4: Golden Models]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-4.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-4.png 1000w, https://aba-blog.xyz/content/images/2022/01/image-4.png 1331w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image-5.png" width="1269" height="869" loading="lazy" alt="DNN &#x2192; Chip Design [2/4: Golden Models]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image-5.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image-5.png 1000w, https://aba-blog.xyz/content/images/2022/01/image-5.png 1269w" sizes="(min-width: 720px) 720px"></div></div></div></figure><p>Once I&apos;ve understood the model inside-out, I start designing the hardware on the whiteboard.</p><h2 id="next">Next:</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-3/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [3/4: Digital Design]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper Whiteboard: Design hardware RTL Design: SystemVerilog/Verilog for the whiteboard designs Generate Test Vectors: using Python Notebooks Testbenc&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [2/4: Golden Models]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/Data-Flow-of-Inference-Engine-2-1.png" alt="DNN &#x2192; Chip Design [2/4: Golden Models]"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[DNN → Chip Design [1/4: Intro & Directory Structure]]]></title><description><![CDATA[<h2 id="kraken-engine">Kraken Engine</h2><p>In March 2020, I started building Kraken as a personal, passion project, an engine capable of accelerating any kind of Deep Neural Network (DNN) with convolutional layers, fully-connected layers, and matrix products using a single uniform dataflow pattern while consuming remarkably low on-chip area. I synthesized it in</p>]]></description><link>https://aba-blog.xyz/dnn-to-chip-1/</link><guid isPermaLink="false">61f418fd33068f34ce882a77</guid><category><![CDATA[Technical Projects]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Fri, 28 Jan 2022 16:57:48 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2022/01/cov-14.jpg" medium="image"/><content:encoded><![CDATA[<h2 id="kraken-engine">Kraken Engine</h2><img src="https://aba-blog.xyz/content/images/2022/01/cov-14.jpg" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><p>In March 2020, I started building Kraken as a personal, passion project, an engine capable of accelerating any kind of Deep Neural Network (DNN) with convolutional layers, fully-connected layers, and matrix products using a single uniform dataflow pattern while consuming remarkably low on-chip area. I synthesized it in 65-nm CMOS technology at 400 MHz, packing 672 MACs in 7.3 mm2, with a peak performance of 537.6 Gops.</p><p>When benchmarked on AlexNet [336.6 fps], VGG16 [17.5 fps], and ResNet-50 [64.2 fps] (yeah, those are old, but those are the widely used benchmarks), it outperforms the state-of-the-art ASIC architectures in terms of overall performance efficiency, DRAM accesses, arithmetic intensity, and throughput, with 5.8x more Gops/mm2 and 1.6x more Gops/W.</p><p>I submitted the design as a &#xA0;journal paper to IEEE TCAS-1, the #3 journal in the field, and it is currently under review. You can find the paper at the following link.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://arxiv.org/abs/2112.02793"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Kraken: An Efficient Engine with a Uniform Dataflow for Deep Neural Networks</div><div class="kg-bookmark-description">Deep neural networks (DNNs) have been successfully employed in a multitude ofapplications with remarkable performance. As such performance is achieved at asignificant computational cost, several embedded applications demand fast andefficient hardware accelerators for DNNs. Previously proposed app&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.arxiv.org/static/browse/0.3.2.8/images/icons/favicon.ico" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><span class="kg-bookmark-author">arXiv.org</span><span class="kg-bookmark-publisher">G Abarajithan</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"></div></a></figure><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image_o-2-1.png" width="1062" height="1042" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image_o-2-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image_o-2-1.png 1000w, https://aba-blog.xyz/content/images/2022/01/image_o-2-1.png 1062w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/quant-2.jpg" width="1280" height="720" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/quant-2.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/quant-2.jpg 1000w, https://aba-blog.xyz/content/images/2022/01/quant-2.jpg 1280w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/kraken2-2.png" width="959" height="908" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/kraken2-2.png 600w, https://aba-blog.xyz/content/images/2022/01/kraken2-2.png 959w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/kraken31-1.png" width="1092" height="1308" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/kraken31-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/kraken31-1.png 1000w, https://aba-blog.xyz/content/images/2022/01/kraken31-1.png 1092w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/vlcsnap-2022-01-30-00h01m19s402-1.png" width="1448" height="724" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/vlcsnap-2022-01-30-00h01m19s402-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/vlcsnap-2022-01-30-00h01m19s402-1.png 1000w, https://aba-blog.xyz/content/images/2022/01/vlcsnap-2022-01-30-00h01m19s402-1.png 1448w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image_o-3-1.png" width="723" height="862" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image_o-3-1.png 600w, https://aba-blog.xyz/content/images/2022/01/image_o-3-1.png 723w" sizes="(min-width: 720px) 720px"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/sys-5.jpg" width="2000" height="1125" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/sys-5.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/sys-5.jpg 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/sys-5.jpg 1600w, https://aba-blog.xyz/content/images/size/w2400/2022/01/sys-5.jpg 2400w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/image_o-12_o-1.png" width="1920" height="1042" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/image_o-12_o-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2022/01/image_o-12_o-1.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2022/01/image_o-12_o-1.png 1600w, https://aba-blog.xyz/content/images/2022/01/image_o-12_o-1.png 1920w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2022/01/c-1.png" width="980" height="911" loading="lazy" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]" srcset="https://aba-blog.xyz/content/images/size/w600/2022/01/c-1.png 600w, https://aba-blog.xyz/content/images/2022/01/c-1.png 980w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>An overview of the series</figcaption></figure><h2 id="technologies-used">Technologies Used</h2><p>Throughout the project, I wrote code, moved back and forth, and interfaced between several technologies, from different domains such as hardware (RTL), software, and machine vision.</p><ul><li>Python - Numpy, Tensorflow, PyTorch</li><li>SystemVerilog - RTL, Testbenches</li><li>TCL - Scripting the Vivado project, ASIC synthesis</li><li>C++ - Firmware to control the system-on-chip</li><li>Tools: Xilinx (Vivado, SDK), Cadence (Genus, Innovus)...</li></ul><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">My Workflow</h2></div><p>Through the Kraken project, I developed a workflow of 15 steps, which helps me to move between golden models in python, RTL designs &amp; simulations in SystemVerilog, and firmware in C++. &#xA0;I have written them in detail as three more blog posts with code examples. </p><h3 id="24-golden-models">2/4: Golden Models</h3><!--kg-card-begin: markdown--><ol>
<li><strong>PyTorch/TensorFlow</strong>: Explore DNN models, quantize &amp; extract weights</li>
<li><strong>Golden Model in Python (NumPy stack)</strong>: Custom OOP framework, process the weights, convert to custom datatypes</li>
</ol>
<!--kg-card-end: markdown--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-2/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [2/4: Golden Models]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper PyTorch/TensorFlow: Explore DNN models, quantize &amp;amp; extract weights Golden Model in Python (NumPy stack): Custom OOP framework, process the&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"></div></a></figure><h3 id="34-digital-design">3/4: Digital Design</h3><!--kg-card-begin: markdown--><ol start="3">
<li><strong>Whiteboard</strong>: Design hardware</li>
<li><strong>RTL Design</strong>: SystemVerilog/Verilog for the whiteboard designs</li>
<li><strong>Generate Test Vectors</strong>: using Python Notebooks</li>
<li><strong>Testbenches</strong>: SystemVerilog OOP testbenches to read the input vector (txt file), randomly control the valid &amp; ready signals and get output vectors (txt files)</li>
<li><strong>Debug</strong>: Python notebooks to compare the expected output with simulation output and to find which dimensions have errors.</li>
<li><strong>Microsoft Excel</strong>: I manually simulate the values in wires with excel to debug</li>
<li>Repeat 3-8: For every module &amp; every level of integration</li>
<li><strong>ASIC Synthesis</strong></li>
</ol>
<!--kg-card-end: markdown--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-3/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [3/4: Digital Design]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper Whiteboard: Design hardware RTL Design: SystemVerilog/Verilog for the whiteboard designs Generate Test Vectors: using Python Notebooks Testbenc&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/Data-Flow-of-Inference-Engine-2-1.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"></div></a></figure><h3 id="44-system-on-chip">4/4: System-on-Chip</h3><!--kg-card-begin: markdown--><ol start="11">
<li><strong>SoC Block Design</strong>: Build FPGA projects with Vivado manually and synthesize</li>
<li><strong>Automation</strong>: TCL scripts to automate the project building and configuration</li>
<li><strong>C++ Firmware</strong>: To control the custom modules</li>
<li><strong>Hardware Verification</strong>: Test on FPGA, compare output to golden model</li>
<li>Repeat 11-14</li>
</ol>
<!--kg-card-end: markdown--><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-4/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [4/4: System-on-Chip]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper SoC Block Design: Build FPGA projects with Vivado manually and synthesize Automation: TCL scripts to automate the project building and configur&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/image_o-12-1.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"></div></a></figure><hr><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" style data-kg-background-image><h2 class="kg-header-card-header">Directory Structure</h2></div><p>Simpler FPGA/ASIC projects can be done within a single folder. However, as the scope and complexity of the project grow, the need to work with multiple languages (Python, SystemVerilog, C, TCL...) &#xA0;and tools (Jupyter, Vivado, SDK, Genus, Innovus...) can grow out of control. </p><p>The project needs to be version-controlled (git) as well, to prevent data loss and to move between different stages of development like a time machine. However, FPGA and ASIC tools create a lot of internal files, which do not generalize between machines. Therefore, the building of such projects is automated via TCL scripts, and only such scripts and the source files are git-tracked.</p><p>The following is the structure I developed through my Kraken project.</p><pre><code>kraken
&#x2502;
&#x251C;&#x2500;&#x2500; hdl
&#x2502;   &#x251C;&#x2500;&#x2500; src        : rtl designs (SystemVerilog/Verilog)
&#x2502;   &#x251C;&#x2500;&#x2500; tb         : SystemVerilog testbenches
&#x2502;   &#x251C;&#x2500;&#x2500; include    : V/SV files with macros
&#x2502;   &#x2514;&#x2500;&#x2500; external   : open-source SV/V libraries
&#x2502;
&#x251C;&#x2500;&#x2500; fpga
&#x2502;   &#x251C;&#x2500;&#x2500; scripts    : FPGA scripts to build &amp; configure projects from source
&#x2502;   &#x251C;&#x2500;&#x2500; projects   : Vivado projects [not git tracked]
&#x2502;   &#x2514;&#x2500;&#x2500; wave       : waveform scripts (wcfg)
&#x2502;
&#x251C;&#x2500;&#x2500; asic
&#x2502;   &#x251C;&#x2500;&#x2500; scripts    : TCL scripts for synth, p&amp;r from source
&#x2502;   &#x251C;&#x2500;&#x2500; reports    : reports from asic tools
&#x2502;   &#x251C;&#x2500;&#x2500; work       : working folder for ASIC tools, [not git tracked]
&#x2502;   &#x251C;&#x2500;&#x2500; log        : [not git tracked]
&#x2502;   &#x2514;&#x2500;&#x2500; pdk        : technology node files, several GBs [not git tracked]
&#x2502;
&#x251C;&#x2500;&#x2500; python
&#x2502;   &#x251C;&#x2500;&#x2500; dnns       : TensorFlow, Torch, TfLite extraction
&#x2502;   &#x251C;&#x2500;&#x2500; framework  : Custom framework
&#x2502;   &#x2514;&#x2500;&#x2500; golden     : Golden models
&#x2502;
&#x251C;&#x2500;&#x2500; data           : [not git tracked]
&#x2502;   &#x251C;&#x2500;&#x2500; input      : input test vectors, text files, generated by python scripts
&#x2502;   &#x251C;&#x2500;&#x2500; output_exp : expected output vectors
&#x2502;   &#x251C;&#x2500;&#x2500; output_sim : output vectors from hardware simulation
&#x2502;   &#x2514;&#x2500;&#x2500; output_fpga: output vectors from FPGA
&#x2502;
&#x251C;&#x2500;&#x2500; cpp
&#x2502;   &#x251C;&#x2500;&#x2500; src        : C++ firmware for the controller
&#x2502;   &#x251C;&#x2500;&#x2500; include    : header files
&#x2502;   &#x2514;&#x2500;&#x2500; external   : external libraries
&#x2502;
&#x2514;&#x2500;&#x2500; doc            : documentation: excel files, drawings...</code></pre><h2 id="next">Next:</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-2/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN &#x2192; Chip Design [2/4: Golden Models]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper PyTorch/TensorFlow: Explore DNN models, quantize &amp;amp; extract weights Golden Model in Python (NumPy stack): Custom OOP framework, process the&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" alt="DNN &#x2192; Chip Design [1/4: Intro &amp; Directory Structure]"></div></a></figure><h2></h2>]]></content:encoded></item><item><title><![CDATA[My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]]]></title><description><![CDATA[<p>I&apos;m no beginner to LaTeX. In fact, our department encourages students to make assignments and reports in LaTeX from the sophomore level. Through countless such submissions, I have tried multiple workflows: TexMaker, Overleaf, VSCode + Latex Workshop and more.</p><p>This year, I began writing my first paper (Kraken), targeting</p>]]></description><link>https://aba-blog.xyz/paper-workflow/</link><guid isPermaLink="false">619680e634a827036b050634</guid><category><![CDATA[Technical Projects]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sun, 21 Nov 2021 06:32:20 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/53-3.png" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/53-3.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"><p>I&apos;m no beginner to LaTeX. In fact, our department encourages students to make assignments and reports in LaTeX from the sophomore level. Through countless such submissions, I have tried multiple workflows: TexMaker, Overleaf, VSCode + Latex Workshop and more.</p><p>This year, I began writing my first paper (Kraken), targeting a journal. Since it was on my personal, passion project, wrote the entire paper, drew all diagrams, made all figures and revised it several times, before handing it over to my advisor, who reviewed it. Because this is my first paper, I wanted everything: the diagrams, graphs, their text styles and all to be perfect. I experimented and learned a lot on the way to craft my own workflow for this and my future publications.</p><p>In a nutshell,</p><ul><li>I draw diagrams with Inkscape, save them as EPS (without text) + TEX (text only), such that text is rendered in latex for a uniform style. </li><li>I use the python ecosystem to handle data. Pandas for spreadsheets, Matplotlib and Seaborn for beautiful plots. SciencePlots to conform to IEEE style.</li><li>I manage my references with Mendeley, sync them across my devices. I use Resilo to sync my handwritten notes on papers between my tablet and laptop</li><li>I write pure LaTeX with VSCode + LaTeX workshop, compile it with MiKTeX, version control everything with git and sync it to overleaf via GitHub for my supervisor.</li></ul><h2 id="drawingsinkscape">Drawings - Inkscape</h2><p>Which one looks better?</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/Figure-8---conv_unit--2--1.png" width="692" height="472" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/Figure-8---conv_unit--2--1.png 600w, https://aba-blog.xyz/content/images/2021/11/Figure-8---conv_unit--2--1.png 692w"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/kraken-1.png" width="1291" height="868" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/kraken-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/kraken-1.png 1000w, https://aba-blog.xyz/content/images/2021/11/kraken-1.png 1291w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>LEFT: one I drew with draw.io for our patent, RIGHT: one I drew with Inkscape for my paper</figcaption></figure><p>Tools like <a href="https://app.diagrams.net/">draw.io</a> lack precision. Also, the size and style of their fonts do not match the rest of the LaTeX text. Inkscape <a href="https://inkscape.org/release/inkscape-1.1.1/">[get]</a> is a professional vector graphics tool, a fully-functional, lightweight, free and open-source alternative to Adobe Illustrator. Therefore, I learnt it to draw the two diagrams in the paper. </p><p>First, find the column/text width of your LaTeX document with <strong>\the\columnwidth</strong><em> </em>in points (1 pt = 1/72.27 inch)<em>.</em> Then set the Inkscape canvas size in <em>File &gt; Document Properties &gt; Page &gt; Custom Size.</em> You can create a grid, either rectangular or isometric, in <em>File &gt; Document Properties &gt; Grid</em>. You can start drawing with Bezier Curve tool [B], organize the drawing by layers...etc</p><p></p><h3 id="adding-text">Adding Text</h3><p>Text can be added in multiple ways using the Text tool [T]. You can simply add text, but that will get rendered with the image, resulting in any font/size you want. But I wanted the text to be rendered in LaTeX, such that it has uniform size and style, to match the rest of the document. For this, I type the text, with any required latex symbols as follows, and save a copy of the image as EPS+LaTeX. This creates an .eps file without text and a .tex file with text only. Make sure to keep the master copy of your drawing updated in the default SVG format (only save a copy in the EPS+TEX format) to avoid data loss.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/11-1.png" width="606" height="704" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/11-1.png 600w, https://aba-blog.xyz/content/images/2021/11/11-1.png 606w"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/12.png" width="459" height="543" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"></div></div></div><figcaption>(1) select the text, adjust the alignment. Inkscape has a bug here. You need to check few times that it is aligned correctly. (2) <em>File &gt; Save a Copy &gt; EPS &gt; Omit Text in PDF</em>. This will create a pair of files.</figcaption></figure><p>Then add the tex file into your LaTeX document using either include or import:</p><figure class="kg-card kg-image-card kg-width-full"><img src="https://aba-blog.xyz/content/images/2021/11/13-1.png" class="kg-image" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" loading="lazy" width="1470" height="492" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/13-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/13-1.png 1000w, https://aba-blog.xyz/content/images/2021/11/13-1.png 1470w"></figure><p>Since this renders text seperately, the text might have moved slightly. Make nessasary adjustments, and recheck the text alignment to make it work.</p><p>The first figure (this one) took me over 10 days to make. To learn inkscape from scratch, try like five different text insertion methods and finally get it right. The next figure (following one) took me just a few hours.</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2021/11/21.png" class="kg-image" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" loading="lazy" width="1637" height="903" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/21.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/21.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2021/11/21.png 1600w, https://aba-blog.xyz/content/images/2021/11/21.png 1637w" sizes="(min-width: 720px) 720px"></figure><h2 id="line-bar-pie-chartspython-ecosystem">Line, bar, pie charts - Python Ecosystem</h2><p>If you prefer Matlab, skip this. I, on the other hand, love the python stack. Numpy is the best in manipulating like 6,7-dimensional arrays, and the object-oriented and intuitive nature of python libraries make them a treat to use. I also found this awesome package: SciencePlots <a href="https://github.com/garrettj403/SciencePlots">[get]</a>, which works on top of other packages and makes IEEE &amp; Science style plots with ease.</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/image-17.png" width="969" height="744" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/image-17.png 600w, https://aba-blog.xyz/content/images/2021/11/image-17.png 969w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/image-18.png" width="900" height="698" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/image-18.png 600w, https://aba-blog.xyz/content/images/2021/11/image-18.png 900w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>Science + Nature vs Science + IEEE styles</figcaption></figure><p>To build the spreadsheet, I populate a pandas dataframe with some regular python code. Yes, I had to spend a few days learning pandas for this, but it&apos;s a great investment. Then I use matplotlib to make plots and seaborn to make them prettier. </p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/32.png" width="521" height="825" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/31.png" width="684" height="860" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/31.png 600w, https://aba-blog.xyz/content/images/2021/11/31.png 684w"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/34.png" width="797" height="649" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/34.png 600w, https://aba-blog.xyz/content/images/2021/11/34.png 797w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/33.png" width="607" height="616" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/33.png 600w, https://aba-blog.xyz/content/images/2021/11/33.png 607w"></div></div></div><figcaption>My code</figcaption></figure><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/35.png" width="1283" height="955" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/35.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/35.png 1000w, https://aba-blog.xyz/content/images/2021/11/35.png 1283w" sizes="(min-width: 1200px) 1200px"></div></div></div><figcaption>Drawings in my paper</figcaption></figure><h2 id="reference-managementmendeley">Reference Management - Mendeley</h2><p>I had to read and summarize like hundred papers for a literature review. Keeping them organized is a nightmare with regular file manager. Which one do you prefer among the following?</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/41.png" width="1095" height="832" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/41.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/41.png 1000w, https://aba-blog.xyz/content/images/2021/11/41.png 1095w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/42.png" width="1538" height="1042" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/42.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/42.png 1000w, https://aba-blog.xyz/content/images/2021/11/42.png 1538w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/43-1.png" width="1342" height="1042" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/43-1.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/43-1.png 1000w, https://aba-blog.xyz/content/images/2021/11/43-1.png 1342w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>(1) Regular file manager (2,3) Mendeley</figcaption></figure><p>Mendeley is available on Web, Windows, Mac and iOS. It has tons of features, including the following.</p><ul><li>Add references as you browse the internet. Install the Mendeley extension and click it from arXiv, IEEE, ResearchGate, Springer, raw PDF...etc. It will fetch the data (author, journal, year, abstract, possibly pdf) and save it to its Web version. You can sync that to all your devices. </li><li>Group by labels, sort by author/year...etc. </li><li>Search within your references fast</li><li>Highlight and add comments using the built-in PDF tool. You may take notes as well. Everything will be synced between all your devices.</li><li>Easily export bibliography as LaTeX. Select a set of references, right-click, copy as BibTeX, then paste into your .bib file.</li></ul><p>I summarize the papers into a Google / Online Excel Sheet, so my supervisor also can have a look.</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2021/11/51.png" class="kg-image" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" loading="lazy" width="1911" height="869" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/51.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/51.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2021/11/51.png 1600w, https://aba-blog.xyz/content/images/2021/11/51.png 1911w" sizes="(min-width: 720px) 720px"></figure><h2 id="sync-handwriting-between-tablet-and-pc">Sync Handwriting between Tablet and PC</h2><p>I prefer drawing on the papers and writing stuff on them with my Android tablet and pen. Mendeley does not work with Android and I&apos;m not sure if its iOS version supports handwriting. Therefore, to sync documents, I use Resilio Sync. It can be installed in both PC <a href="https://www.resilio.com/platforms/desktop/">[get]</a> and tablet <a href="https://www.resilio.com/platforms/mobile/">[get]</a>. Folders from the PC can be connected to those in the tablet, such that any changes made on either are reflected in another one within minutes.</p><p>I use Xodo <a href="https://play.google.com/store/apps/details?id=com.xodo.pdf.reader&amp;hl=en&amp;gl=US">[get]</a>, an excellent PDF manager on my tablet to draw on the papers. These then reflect in the PC (and within Mendeley).</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://aba-blog.xyz/content/images/2021/11/43-2.png" width="1342" height="1042" loading="lazy" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/43-2.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/43-2.png 1000w, https://aba-blog.xyz/content/images/2021/11/43-2.png 1342w" sizes="(min-width: 1200px) 1200px"></div></div></div></figure><h2 id="pure-latex-but-fastervscode">Pure LaTeX, but faster! - VSCode</h2><blockquote>&quot;What&apos;s wrong with Overleaf?&quot; <br>&quot;Oh you&apos;re a simpleton happy with LyX!&quot; &#xA0;</blockquote><p>Nope, I write pure LaTeX. Overleaf is great, but I hate web-based tools. I prefer something local, more responsive (thanks to my shitty connection) and with proper version control. Since I wrote the entire journal paper alone, all 15 pages of it, and revised it like ten times, I had the freedom to develop my own workflow. When I finally passed it on to my supervisor for final revision, I synced it with Overleaf so he can work with it comfortably and his changes will be reflected in my local machine. To compile LaTeX locally, you need a compiler like MiKTeX: <a href="https://miktex.org/">miktex.org</a>.</p><h3 id="vscode-latex-workshop">VSCode + Latex Workshop</h3><p>Maintained by Microsoft, yet free and open-source, VSCode <a href="https://code.visualstudio.com/download">[get]</a> is the best editor/IDE out there for almost any language, hands-down. Unless you are a vim user, of course. By installing the right extensions, you can make it into a super-powerful, IDE of any language. Latex Workshop <a href="https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop">[get]</a> is the extension we need. Coupled with VScode&apos;s native features, it provides a killer LaTeX experience. Check out some of its coolest features below. More features are listed <a href="https://marketplace.visualstudio.com/items?itemName=James-Yu.latex-workshop">here</a>. The theme I use is One Dark Pro Monokai Darker <a href="https://marketplace.visualstudio.com/items?itemName=eserozvataf.one-dark-pro-monokai-darker">[get]</a>.</p><figure class="kg-card kg-image-card kg-width-full kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2021/11/53-2.png" class="kg-image" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" loading="lazy" width="1920" height="1042" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/53-2.png 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/53-2.png 1000w, https://aba-blog.xyz/content/images/size/w1600/2021/11/53-2.png 1600w, https://aba-blog.xyz/content/images/2021/11/53-2.png 1920w"><figcaption>Multiple tabs, symbols pane, preview equations on hover, ultra-fast intelligent autocomplete, easy-on-eyes dark theme, pretty colors (theme can be chosen independently), seamless git</figcaption></figure><h3 id="version-controlgit">Version Control - git</h3><p>Git is a pain to a lot of people. It was the same to me, until mid-2020. When I was stranded in New Delhi due to COVID lockdowns amidst my solo backpacking, I started reading some books on git. Then I figured that it&apos;s what I have been missing all my life.</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2021/11/image-20.png" class="kg-image" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]" loading="lazy" width="330" height="478"></figure><p>Git is a version control tool. Basically, a time machine that helps you to move to any point in the development, compare two points (diff), manage them collaboratively...etc. It guarantees that &quot;commits&quot; (aka snapshots) of your folder are immutable and non-deletable, giving you the confidence to clean up your code and try new things after each commit. &#xA0;</p><p>It has a horrible interface. The commands barely make sense. But underneath, it has a beautiful data model. Once I understood that deeply, I was able to recall or google commands at will and use git like a pro. Now I&apos;m addicted to the safety it provides. Even for a personal project, I start tracking it with git. Note, a git is a local tool. You don&apos;t need an internet connection to use it.</p><figure class="kg-card kg-embed-card kg-card-hascaption"><iframe width="200" height="113" src="https://www.youtube.com/embed/2sjqTHE0zok?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><figcaption>An awesome MIT video that explains git beautifully. This video changed my life.</figcaption></figure><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://missing.csail.mit.edu/2020/version-control/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Version Control (Git)</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://missing.csail.mit.edu/apple-touch-icon.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"><span class="kg-bookmark-author">the missing semester of your cs education</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://imgs.xkcd.com/comics/git.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"></div></a><figcaption>Corresponding article to the video</figcaption></figure><p>Okay, how do I use git for paper-writing? Well, simple. Once you understand git, start tracking all your TEX, EPS, SVG, BIB, python, ipynb files using git. You will feel confidence and power. When you add something and save (recompile) successfully, make a commit with an appropriate commit message (eg: &quot;Modify table horz_conv, to add circles around y&quot;).</p><h3 id="overleaf-sync">Overleaf Sync</h3><p>Right, all good. But my supervisor is not familiar with git and Mendeley. What do I do? He is very familiar with overleaf though. So, I set up sync to overleaf via Github. I had to purchase a student account for $8/month. </p><p>First, create a GitHub repo (remote), connect overleaf to that empty repo, push your local repo to remote, then import changes into overleaf. When the supervisor changes something, I can push that from overleaf to GitHub and pull it into the local repo. Following is a tutorial</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://www.overleaf.com/learn/how-to/How_do_I_connect_an_Overleaf_project_with_a_repo_on_GitHub%2C_GitLab_or_BitBucket%3F#:~:text=You%20can%20configure%20your%20Overleaf,Then%20follow%20the%20prompts."><div class="kg-bookmark-content"><div class="kg-bookmark-title">How do I connect an Overleaf project with a repo on GitHub, GitLab or BitBucket?</div><div class="kg-bookmark-description">An online LaTeX editor that&#x2019;s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://www.overleaf.com/touch-icon-192x192.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"><span class="kg-bookmark-author">Overleaf, Online LaTeX Editor</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://cdn.overleaf.com/img/ol-brand/overleaf_og_logo.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"></div></a></figure><h2 id="conclusion">Conclusion</h2><p>Yeah, I had to learn 80% of this over several weeks to set up my workflow. But I believe this is a worthy investment, as I can continue to write future papers with this easily.</p><p>If you think I am taking this too far, here&apos;s what I aspire to be. This mathematics freshman takes lecture notes with LaTeX using vim (yep) while drawing all mathematical drawings in real-time (as the prof draws on blackboard) on Inkscape.</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://castel.dev/post/lecture-notes-1/"><div class="kg-bookmark-content"><div class="kg-bookmark-title">How I&#x2019;m able to take notes in mathematics lectures using LaTeX and Vim</div><div class="kg-bookmark-description">A while back I an&#xAD;swered a ques&#xAD;tion on Quora: Can peo&#xAD;ple ac&#xAD;tu&#xAD;al&#xAD;ly keep up with note-&#x200B;taking in Math&#xAD;e&#xAD;mat&#xAD;ics lec&#xAD;tures with LaTeX.There, I ex&#xAD;plained my work&#xAD;flow of tak&#xAD;ing lec&#xAD;ture notes in LaTeX using Vim and how I draw fig&#xAD;ures in Inkscape.How&#xAD;ev&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://castel.dev/icons/icon-512x512.png?v=27cd7f782269f5d5087006b570d8d8c1" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"><span class="kg-bookmark-author">Gilles Castel</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://castel.dev/static/ba66c52b01c8fe6b31c97af52b5eb7f7/f9fff/ca1.png" alt="My Paper-Writing Workflow [Inkscape, Python, Mendeley, VSCode, Git]"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[Afghanistan: A Sad Story]]></title><description><![CDATA[<p>TLDR of some counterintuitive facts: &#x200B;</p><ul><li>77% of Afghan people supported the US invasion.&#x200B;</li><li>The US invaded to avenge 9/11. There&#x2019;s not enough oil there.&#x200B;</li><li>The US did war crimes, some serious development but failed to build a self-sustaining nation as they got distracted with</li></ul>]]></description><link>https://aba-blog.xyz/afghanistan-a-sad-story/</link><guid isPermaLink="false">61990b0234a827036b05065c</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sat, 07 Aug 2021 14:53:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/afg.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/afg.jpg" alt="Afghanistan: A Sad Story"><p>TLDR of some counterintuitive facts: &#x200B;</p><ul><li>77% of Afghan people supported the US invasion.&#x200B;</li><li>The US invaded to avenge 9/11. There&#x2019;s not enough oil there.&#x200B;</li><li>The US did war crimes, some serious development but failed to build a self-sustaining nation as they got distracted with Iraq in 2003.&#x200B;</li><li>Afghan people have been intercours&apos;ed by the British, Soviets, local tribal leaders, Taliban and US, one after the other.</li><li>Taliban means &quot;student&quot; in Arabic. They were the fatherless refugee children orphaned in the Soviet war, raised in Wahhabist schools of Pakistan funded by Saudi Arabia.&#x200B;</li><li>Taliban committed at least 15 massacres and genocides targeting Shia and Hazara (Muslims) when they were in power. This is why people are desperate to flee.</li><li>Taliban assassinated the one honest leader (Massoud) who promoted peaceful democracy, as they couldn&apos;t bribe him.&#x200B;</li><li>Trump cut a secret deal with the Taliban to withdraw by May 1st. &#x200B;</li><li>Afghan state failed because military was &#x201C;incompetent fools, corrupt to the patrol level&#x201D;, 30% of police became bandits, public lost trust in govt.&#x200B;</li></ul><p>Afghanistan is made of a lot of different ethnicities and tribes in rural areas scattered across mountains. British drew their borders arbitrarily. Their biggest ethnicity: Pashtun is split between Pakistan and Afghanistan. So people don&#x2019;t respect that border and keep moving back and forth. This allowed the Taliban to regroup and train in Pakistan easily. Khyber pass, an extremely strategic path through Hindu Kush mountains through which, Indo-Aryans, Genghis Khan, Persians, Mughals and British invaded India, was later used by American troops and Taliban to enter Afghanistan. &#x200B;</p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/Ab9zK8yT4_Y?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>Following British independence in 1919, the king tried to modernize the country by educating everyone (including women) and abolishing women&apos;s face veil. These liberal reforms led to a civil war with tribal leaders. After few more kings, in 1964 Afghanistan became more democratic, allowing multiple parties. One party (PDPA) took the power in a non-violent coup and started implementing communism, with the support of Soviet Russia.&#x200B;<br>&#x200B;<br>That &quot;democratic party&quot; banned forced marriages and promoted the education &amp; job security of women. They also tortured and killed local Muslim leaders and forced atheism. This and dependence on the Soviet Union angered the people. Islamist riots by Afghan Mujahedeen (armed tribal leaders) broke out. The Soviet Union invaded to quell the rebellion, killed millions and raped women. They systematically depopulated rural areas with landmines and even toy grenades targeted at children. In response, the US supplied anti-aircraft guns to Mujahedeen. After 9 years, in 1989, as USSR collapsed, the Soviets withdrew in defeat, much like the US does now. &#x200B;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://en.wikipedia.org/wiki/History_of_Afghanistan"><div class="kg-bookmark-content"><div class="kg-bookmark-title">History of Afghanistan - Wikipedia</div><div class="kg-bookmark-description"></div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://en.wikipedia.org/static/apple-touch/wikipedia.png" alt="Afghanistan: A Sad Story"><span class="kg-bookmark-author">Wikimedia Foundation, Inc.</span><span class="kg-bookmark-publisher">Contributors to Wikimedia projects</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://upload.wikimedia.org/wikipedia/commons/2/24/Shuja_Shah_Durrani_of_Afghanistan_in_1839.jpg" alt="Afghanistan: A Sad Story"></div></a></figure><p>People cheered, considering Mujahedeen as liberators. They then became warlords, tore up their areas and started a civil war between themselves. Half of Kabul was reduced to rubble. UN tried to form a coalition govt of them in 1992 and failed. Fatherless children from the Soviet war grew up in refugee camps in Pakistan, and were indoctrinated into wahhabism by madarasas funded by Saudi Arabia. They called themselves Taliban, that is students. Pakistan opened the refugee camps, in 1994, they invaded Afghanistan. &#x200B; People welcomed them as liberators. </p><figure class="kg-card kg-embed-card"><iframe width="200" height="113" src="https://www.youtube.com/embed/zzBVvyBWDD4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></figure><p>In 1996, the Taliban seized Kabul with the support of Saudi Arabia and Pakistan and brought the sharia rule. They forbade women from leaving homes and studying. They committed at least 15 systematic massacres and genocides targeting the Shia and Hazara, torturing and killing 4000 at a time. Ahmed Shah Massoud, a respected tribal leader, railed up enough opposition, set up democratic institutions and promoted women rights. Taliban tried to bribe him by giving the Prime Minister position, but he declined and asked for a democratic solution. He addressed the EU in 2001, stating that the Taliban and Al Qaeda had introduced &quot;a very wrong perception of Islam&quot;. The Taliban assassinated him in 2001.&#x200B;<br>&#x200B;<br>On 11/9/2001, Afghan-based Al-Qaeda attacked the world trade centre. Taliban gave Osama Bin Laden safe harbour and in return US-led forces invaded Afghanistan. There isn&#x2019;t enough oil there though. In December 2001, the Taliban government was toppled. 77% of Afghan people supported the American invasion <a href="acsor-surveys.com/wp-content/uploads/2015/01/Afghan-Futures-Wave-6-Analysis_FINAL-v2.pdf">[source 1]</a> <a href="https://drum.lib.umd.edu/bitstream/handle/1903/10127/Afghanistan_Jan06_art2.pdf">[source 2]</a>.&#x200B;<br>&#x200B;<br>NATO started building a government <a href="https://apnews.com/article/joe-biden-middle-east-afghanistan-8d6d6d9e1e7cddbd49caf0b52a40c2e8">[source]</a>, training the Afghan army and police and implementing reforms. Women education rose from 0% to 60% (2016), free media and platforms for public debate were established. Infant mortality rates fell by half. In 2005, fewer than 1 in 4 Afghans had access to electricity. By 2019, nearly all did. The Afghan geography makes centralized control impossible. Most of the country being over 2000m in elevation makes the road networks non-existent. The US started rebuilding the Soviet-built circular highway with the help of the world bank, Saudi Arabia and Iran (wow), to unify the country. Then they got occupied with Iraq, Taliban strengthened themselves and started blowing up the highway and construction workers.&#x200B;<br>&#x200B;<br>Afghan society is geared more towards family and tribe than having a &#x201C;unified afghan&#x201D; feeling. The popular people who got Afghan govt positions were corrupt tribal warlords. In the leaked secret documents, the US military officials call Afghan soldiers &quot;incompetent fools and corrupt to the patrol level&#x201D; and blame themselves &#x201C;we moved too slowly initially when Taliban were defeated. When they rebounded, we trained too quickly&#x201D; (and quality fell) <a href="https://www.washingtonpost.com/graphics/2019/investigations/afghanistan-papers/afghanistan-war-confidential-documents/">[source]</a>. The US built an Afghan military that dwarfs the military of even developed countries. The US spent 20 years and 2 trillion dollars there, which mostly went to corrupt Afghan officers. 30% of the new police escaped with their weapons to put up their private checkpoints, becoming bandits, robbing people. The rural public lost trust in the centralized govt, police and military.&#x200B;<br>&#x200B;<br>Meanwhile, the Taliban regrouped in Pakistan and started attacking and controlling the strategic choke points. The US conducted war crimes, torture and killings of Taliban prisoners and suspects. <a href="https://www.hrw.org/news/2021/07/06/how-us-funded-abuses-led-failure-afghanistan">The US killed 9 children in 2003 and 100 civilians (mostly children) in 2009</a>. This turned the international perception and rural Afghans against the US occupation. Of civilian casualties, 40% was due to the US &amp; Afghan govt and 60% was due to the Taliban. &#x200B;<br>&#x200B;<br>After Osama was killed in 2011, the occupation became unpopular among the US public. They didn&#x2019;t want their children being killed in an unrelated conflict. The US govt also did not have a clear objective after killing Osama. As a last attempt, Obama tried a &quot;surge&quot; of troop inflow, but the Taliban just surged their attacks. Obama started pulling out. Trump stroke a secret deal with the Taliban, without even consulting the Afghan govt they built, that the US would pull out by May 1st 2021. He reduced the 15,500 troops to 2,500. Biden says US succeeded in its objective: avenging 9/11 and making sure the region doesn&apos;t breed terrorism targeting mainland USA. Given Afghanistan is now a Taliban controlled terrorist state, US might have failed in that objective too.&#x200B;<br>&#x200B;<br>An exit strategy wasn&#x2019;t planned. When the Taliban attacked this year, with only 2500 US troops, Biden had to either send more resources and restart the war or stop the war by pulling out immediately, he decided on the latter <a href="https://www.rev.com/blog/transcripts/joe-biden-speech-transcript-on-afghanistan-taliban-takeover-august-16">[source]</a>. The US military shut off the lights in their airbase and slipped disgracefully without informing the Afghan army. The president fled the country betraying his people. The incompetent and corrupt Afghan govt and army collapsed like a house of cards and most of them switched sides to the Taliban for bribes.&#x200B;<br>&#x200B;<br>Now people are desperate to flee through the borders and Kabul airport, fearing the extremist rule, genocides and massacres of the Taliban. The US has a deal with the Taliban to continue the evacuation of allies and special visa Afghans. US &amp; French ambassadors fled, but the UK ambassador stays back writing visas for the Afghan translators, helping them escape. China quickly joined hands with the Taliban and India is shocked to have a terrorist country in its footsteps. The future of Afghanistan and its people is as uncertain as ever before.&#x200B;</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2021/11/236882220_10102988939664985_5938514270728019212_n.jpg" class="kg-image" alt="Afghanistan: A Sad Story" loading="lazy" width="474" height="960"></figure><p>&#x200B;&#x200B;Disclaimer: I&apos;m no expert. Feel free to mention any missing points in the comments, will add them.</p><figure class="kg-card kg-bookmark-card kg-card-hascaption"><a class="kg-bookmark-container" href="https://web.facebook.com/abarajithan11/posts/10223120775519253"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Abarajithan Gn</div><div class="kg-bookmark-description">Afghanistan: A Sad Story - Who is responsible?&#x200B;&#x200B;TLDR of some counterintuitive facts: &#x200B;-- 77% of Afghan people supported the US invasion.&#x200B;-- US invaded to avenge 9/11. There&#x2019;s not enough oil...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.xx.fbcdn.net/rsrc.php/yv/r/B8BxsscfVBr.ico" alt="Afghanistan: A Sad Story"><span class="kg-bookmark-author">Facebook</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scontent.fcmb7-1.fna.fbcdn.net/v/t39.30808-6/238876826_10223120772679182_1879016139319862998_n.jpg?_nc_cat=107&amp;ccb=1-5&amp;_nc_sid=8024bb&amp;_nc_ohc=eRDSqi64dokAX975S-3&amp;_nc_ht=scontent.fcmb7-1.fna&amp;oh=c711348e2cb5a486e25787674ad8bcf1&amp;oe=61A04815" alt="Afghanistan: A Sad Story"></div></a><figcaption>First posted on Facebook. Click to see comments &amp; shares</figcaption></figure>]]></content:encoded></item><item><title><![CDATA[How safe are our compilers?]]></title><description><![CDATA[<p>An important argument placed against e-voting machines is that there is absolutely no way to prevent large-scale fraud. Even if the firmware is open-source, how do you verify it&apos;s the same code programmed into the machine? Even if it was compiled in front of your eyes, how do</p>]]></description><link>https://aba-blog.xyz/compiler-vulnerabilities/</link><guid isPermaLink="false">61990ca534a827036b05067b</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Thu, 05 Aug 2021 14:59:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/compiler-issues-compiler-t2mkn8.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/compiler-issues-compiler-t2mkn8.jpg" alt="How safe are our compilers?"><p>An important argument placed against e-voting machines is that there is absolutely no way to prevent large-scale fraud. Even if the firmware is open-source, how do you verify it&apos;s the same code programmed into the machine? Even if it was compiled in front of your eyes, how do u know the compiler itself is safe?&#x200B;<br>&#x200B;<br>GCC, the most popular C/C++ compiler itself is actually written in C. Then how is it compiled? Using older versions of itself! This fact was used by Ken Thompson, the legend who designed and implemented UNIX, the OS on which Linux (hence all android phones, data centers, mars rovers) and macOS (an overpriced piece of shit ;-) ) are built upon.&#x200B;<br>&#x200B;<br>When he wrote the &apos;login&apos; program of UNIX (during early days), he put a backdoor for debugging purposes. That is, given his secret password, any UNIX machine would unlock. But anyone else who reads the source code of login would notice this and panic. So, he hid that backdoor in the compiler, such that if the login program is compiled, it would insert the backdoor into the binary (assembly), else it would compile other programs normally. &#x200B;<br>&#x200B;<br>But, wouldn&apos;t people read compiler&apos;s source code? For that, he first built the backdoor in the compiler roughly as follows. If future compiler code is given to the compiler, it would generate a binary for a rigged compiler with the above back door. Else, it would compile other programs normally. &#x200B;<br>&#x200B;<br>He then compiled the compiler into binary and then removed the backdoor from the compiler&apos;s source code. Now the backdoor is practically invisible. it only exists as 1s and 0s in the binary. Anyone who reads the source code of the compiler sees it&apos;s perfectly fine. But when they add features to future versions of compiler code &amp; compile it with his rigged compiler, it generates a new, rigged compiler binary. When they compile the login source code with that rigged compiler in the future, it would place the back door into the login binary. &#x200B;<br>&#x200B;<br>Anyway, it was temporary, he never got caught and he revealed it when he received the Turing Award. However, this is an interesting phenomenon, and any kind of malware could be injected like this since we usually apt-get / download .exe of the compiler, which was in fact compiled by the older version of the same compiler.&#x200B;<br>&#x200B;<br>Btw, this is pretty common. Recently I heard from a podcast, that few years ago hackers infiltrated a company that makes network-security software. They injected their virus into their development toolchain. In the next release, that got into the software and was distributed to all their customers, who are big software companies and the US govt itself. It was found months later.&#x200B;</p><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://scienceblogs.com/goodmath/2007/04/15/strange-loops-dennis-ritchie-a"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Strange Loops: Ken Thompson and the Self-referencing C Compiler | ScienceBlogs</div><div class="kg-bookmark-description">I&#x2019;m currently reading &#x201C;I am a Strange Loop&#x201D; by Douglas Hofstadter. I&#x2019;ll be posting a review of it after I finish it. A &#x201C;strange loop&#x201D; is Hofstadter&#x2019;s term for a G&#xF6;del-esque self-referential cycle. A strange loop doesn&#x2019;t have to involve G&#xF6;del style problems - any self-referential cycle is a strange l&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://scienceblogs.com/files/favicon.ico" alt="How safe are our compilers?"><span class="kg-bookmark-author">Home</span><span class="kg-bookmark-publisher">goodmath</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scienceblogs.com/files/styles/thumbnail/public/pictures/markcc.jpg?itok=PmLQEFZp" alt="How safe are our compilers?"></div></a></figure><h3 id="first-posted-on-facebook">First posted on Facebook:</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://web.facebook.com/abarajithan11/posts/10223048594314768"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Abarajithan Gn</div><div class="kg-bookmark-description">How safe are our software? - Compiler vulnerabilities&#x200B;&#x200B;An important argument placed against e-voting machines is that there is absolutely no way to prevent large-scale fraud. Even if the firmware...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.xx.fbcdn.net/rsrc.php/yv/r/B8BxsscfVBr.ico" alt="How safe are our compilers?"><span class="kg-bookmark-author">Facebook</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scontent.fcmb7-1.fna.fbcdn.net/v/t1.6435-1/124589615_10221159028996816_3199591677153576602_n.jpg?_nc_cat=105&amp;ccb=1-5&amp;_nc_sid=baafbc&amp;_nc_ohc=fDsPEnrpl8gAX-8HYZa&amp;_nc_ht=scontent.fcmb7-1.fna&amp;oh=00_AT8wy-7SW3XvR3KL2cIdqCmdVCn2aUtqA8itZbqgwvPwdA&amp;oe=620DA41F" alt="How safe are our compilers?"></div></a></figure>]]></content:encoded></item><item><title><![CDATA[The Snowflake Generation:  A Defense]]></title><description><![CDATA[<p>&quot;This is such a snowflake generation&quot; has been the mainstream conservative opinion for ages. In every period of history, the elder generation has accused so. It is part of the &quot;during my time, we had to defeat a balrog to get to school&quot; mentality.&#x200B;<br>&#x200B;</p>]]></description><link>https://aba-blog.xyz/snowflake-generation/</link><guid isPermaLink="false">61990db334a827036b05068e</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sun, 27 Jun 2021 15:01:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/snowflakes.1200x675.shutterstock-1536x1024.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/snowflakes.1200x675.shutterstock-1536x1024.jpg" alt="The Snowflake Generation:  A Defense"><p>&quot;This is such a snowflake generation&quot; has been the mainstream conservative opinion for ages. In every period of history, the elder generation has accused so. It is part of the &quot;during my time, we had to defeat a balrog to get to school&quot; mentality.&#x200B;<br>&#x200B;<br>This issue is similar to the memes about safety instructions. 50 years ago they didn&apos;t write &quot;do not drink&quot; on bleach bottles. Today we do. Does this mean we are dumber?&#x200B;<br>&#x200B;<br>No. In any given population, there&apos;s a probability (say 0.001%) of people who are dumb enough to drink bleach. 50 years ago, they simply died. Today we have better consumer protection laws. If one guy dies, media erupts, companies get sued to bankruptcy. By simply making a minor change, which doesn&apos;t affect the lives of 99.999% of the population, we have drastically reduced the number/percentage of the dumb people who actually die. Is that bad?&#x200B;<br>&#x200B;<br>Same with body shaming/sexual harassment...etc. There&apos;s always a percentage who are unable to handle it. I don&apos;t think that percentage is increasing. By spreading awareness, we are reducing the number/percentage of people who will mentally break down, grow up to be deeply scarred adults, or even commit suicide. It doesn&apos;t affect us personally, but we can bear a small inconvenience, to save a few lives. This is good, we should be proud.&#x200B;<br>&#x200B;<br>I&apos;m actually happy about this generation. We see kids being wholesome, having a common sense of what shouldn&apos;t be made fun of. A disabled kid, one with disfiguration is less likely to get bullied at school today. It was shocking to talk to mom and see what they have bullied about, during her time. She was unable to comprehend that we think differently and we don&apos;t even consider those funny. That&apos;s a win for our generation. We are improving. &#x200B; Our younger generation will be better than us.&#x200B;<br>&#x200B;<br>If you want to improve things further, teach your kids to stand up against injustice. Teach them to get into trouble in order to fearlessly defend themselves and the weak around them, regardless of the consequences. And when they do, tell them how proud you are. Teach them not to be blindly loyal to their morally corrupt friends. This is how we can build a resilient society that upholds justice.</p><p>First posted on Facebook: <a href="https://web.facebook.com/abarajithan11/posts/10222787695392458">facebook.com/abarajithan11/posts/10222787695392458</a></p>]]></content:encoded></item><item><title><![CDATA[Tamil 101: For Non-Tamils​]]></title><description><![CDATA[<p>TLDR: &#xA0;<em>Spoken Tamil dialects are just fancy ways of shortening written Tamil. There is one unified written form, with well-defined grammar. But since the verb packs a ton of information as suffixes, which get shortened in spoken dialects, it might be hard to notice that pattern for beginners. Extracted</em></p>]]></description><link>https://aba-blog.xyz/tamil-101/</link><guid isPermaLink="false">61990ef134a827036b05069f</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Wed, 09 Jun 2021 15:09:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/tamil.png" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/tamil.png" alt="Tamil 101: For Non-Tamils&#x200B;"><p>TLDR: &#xA0;<em>Spoken Tamil dialects are just fancy ways of shortening written Tamil. There is one unified written form, with well-defined grammar. But since the verb packs a ton of information as suffixes, which get shortened in spoken dialects, it might be hard to notice that pattern for beginners. Extracted from my convo with Kasun Withana. </em></p><hr><p>Tolkappiyam (literally old epic), written 2200 years ago, defines the grammar of Tamil as used today. The first verse goes like &quot;letters are 30. From a to na&quot;. So there goes the first misconception. Tamil doesn&apos;t have 248 letters. Only 30 prime ones. &#x200B;<br>&#x200B;<br>12 vowels, 18 consonants. &#x200B; The vowels are the ones in English (a,e,i,o,u) + their longer counterparts (5) + &apos;ai&apos; + &apos;ou&apos;. 6 extra consonants are borrowed from Sanskrit (although Tamil is from a different family) to get fancy sounds: sa, sha, ja, ha, ksha, shree. My name has one. There goes the second misconception. Tamil has fancy sounds.&#x200B;<br>&#x200B;<br>Third one: &apos;ae&apos;, &apos;aae&apos; vowels of Sinhala are also there. They are implicit, they arise naturally when compounding with some consonants. But f, z are not there.&#x200B;<br>&#x200B;<br>Unlike some other similarly ancient classical languages (Arabic, Chinese, Greek), the grammar of Tamil has not changed much or diverged into varieties in 2200 years. However, the alphabet has changed. Started as Tamil Brahmi (Brahmi was the common script in India), it changed gradually. After a massive reform in 1978, the letters and their combinations are very consistent now.&#x200B;<br>&#x200B;<br>The spoken dialects are based on written Tamil, whose verbs are a bit complicated. Tamil is an agglutinative language. Unlike spoken Sinhala and English, the tense (past/present/future), sex (male/female/non-conscious), voice (active/passive), number (singular/plural) are all added as suffixes to the verb. This follows a well-defined order and set of rules, and unlike English, exceptions are extremely rare. &#x200B;<br>&#x200B;<br>For example:&#x200B;</p><blockquote>avan saappitt-aan = he ate&#x200B;<br>avan saappittu-vitt-aan = he has eaten&#x200B;<br>avan saappitt-irupp-aan = he would have eaten&#x200B;<br>avan saappittu-kkond-irunth-irupp-aan = he would have been eating&#x200B;<br>avan saappittu-kkond-irunth-irukka-maattaan = he would not have been eating&#x200B;<br>&#x200B;<br></blockquote><p>Breaking the verb:&#x200B;</p><blockquote>saappidu = eat (base verb - command)&#x200B;<br>-vittu- = perfect&#x200B;<br>iru = would&#x200B;<br>ttaan = male, past&#x200B;</blockquote><p>Fun fact: The word &quot;&#xB9A;&#xBC6;&#xBB2;&#xBCD;&#xBB2;&#xBBE;&#xBA4;&#xBBF;&#xBB0;&#xBC1;&#xBAA;&#xBCD;&#xBAA;&#xBB5;&#xBB0;&#xBCD;&quot; (cellaathiruppavar) is ranked 8th in The Most Untranslatable Word In The World. &#x200B;<br>&#x200B;<br>The pattern of suffixes is hard to notice from spoken dialects. Tamil is harder to speak than Sinhala, precisely due to this. Spoken dialects are simply different ways of shortening written Tamil. Jaffna Tamil, SL Muslim Tamil, Kotahena slang, Madras tamil... Tamil Nadu must have hundreds of dialects. If you speak the common written form, you&apos;ll sound like a newsreader or like a modern poet. But it will be intelligible and people will love you for the effort.</p><blockquote>Irukkinraaya? - Written = Are (you) there?&#x200B;<br>irukkiya? - Indian / sms&#x200B;<br>irukkiriya? - Jaffna&#x200B;<br>eekkiyaa? - Muslim&#x200B; (Akurana)</blockquote><p>Madras Tamil is fun. It&apos;s the most optimized form of Tamil. They condense so much information into so few syllables. &#x200B;</p><blockquote>Iluthuththuk kondu po = pull this and go (take it away)&#x200B;<br>isthukinupo - Madras Tamil&#x200B;</blockquote><p>So, yeah... If you would like to learn spoken Tamil, you don&apos;t need to learn the written one. But it helps to know that seemingly arbitrary changes in verbs come from well-defined grammatical rules of written Tamil. You can find a youtube tutorial video of a cute girl and learn to speak from there &#x1F609;.</p><p>First posted on Facebook: <a href="https://web.facebook.com/abarajithan11/posts/10222627017895621">facebook.com/abarajithan11/posts/10222627017895621</a></p>]]></content:encoded></item><item><title><![CDATA[English Skills: Pride & Prejudice]]></title><description><![CDATA[<p>Learning English is not about pride, it&apos;s just another language. I learnt it out of necessity since I was too shy to speak in Sinhala from Grade 7-ALs. But as an international language, English brings a lot of opportunities to get exposure.&#x200B;<br>&#x200B;<br>By the way, being</p>]]></description><link>https://aba-blog.xyz/english-skills/</link><guid isPermaLink="false">619910c134a827036b0506d1</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sun, 06 Jun 2021 00:00:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/shutterstock_343329656.png" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/shutterstock_343329656.png" alt="English Skills: Pride &amp; Prejudice"><p>Learning English is not about pride, it&apos;s just another language. I learnt it out of necessity since I was too shy to speak in Sinhala from Grade 7-ALs. But as an international language, English brings a lot of opportunities to get exposure.&#x200B;<br>&#x200B;<br>By the way, being fluent in English doesn&apos;t mean you have to let go of your first language. In my experience, my bilingual friends (and I) read a lot in both languages, watch good cinema in both languages and have an excellent grasp of the grammar and vocabulary of their native language as well. If your kid grows up speaking English only, the fault is yours, not of the kid or English. &#x200B;<br>&#x200B;<br>To add to the list, you get to learn that:&#x200B;&#x200B;</p><ul><li>Romans had complex cities and an empire spanning Europe with drainage systems when Vijaya was landing in Sri Lanka (6th century BC). They even kept records of what happened in their courts from that time.&#x200B;&#x200B;</li><li>Roman roads, designed as multi-layered, built in 300 BC are still being used. Around that time, Byzantium had multi-story apartment complexes which were rented.&#x200B;&#x200B;</li><li>When Aryans were settling in Ganges valley (3000-4000 BC), Egypt and Indus valley had complex civilizations that were trading over the ocean. Egypt, Assyria and all had houses well planned and built for average citizens. </li></ul><p>So yeah, Sri Lanka is neither the oldest nor the greatest civilization in the world. But we have a rich history of multiple cultures. We need to preserve that and do actual historical research, rather than boasting about legends. Meanwhile, we can appreciate other cultures and civilizations that are similarly great as well.</p><p>First posted on Facebook: <a href="https://web.facebook.com/abarajithan11/posts/10222599976619606">facebook.com/abarajithan11/posts/10222599976619606</a></p>]]></content:encoded></item><item><title><![CDATA[Webinars: Modern C++ & Embedded]]></title><description><![CDATA[<p>Following the success of SystemVerilog session, my friend and fellow junior lecturer Kithmin and few others got together to organize another Missing Semester series, for ROS. When that succeded beyond expectations, Kithmin, myself and Dr. Subodha joined hands with few final years students for <strong>Missing Semester 3: Embedded Systems</strong></p><p>I</p>]]></description><link>https://aba-blog.xyz/embedded-2021/</link><guid isPermaLink="false">6195a85e34a827036b050499</guid><category><![CDATA[Teaching]]></category><category><![CDATA[Community Work]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Tue, 18 May 2021 02:24:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/modern_cpp_2021.png" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/modern_cpp_2021.png" alt="Webinars: Modern C++ &amp; Embedded"><p>Following the success of SystemVerilog session, my friend and fellow junior lecturer Kithmin and few others got together to organize another Missing Semester series, for ROS. When that succeded beyond expectations, Kithmin, myself and Dr. Subodha joined hands with few final years students for <strong>Missing Semester 3: Embedded Systems</strong></p><p>I taught Modern C++ (11+), its features and best practices. We then extended that series into a formal certificate course through University of Moratuwa, and then into a wider, but shallower course for youngsters: Kickstarter on Embedded Systems</p><!--kg-card-begin: html--><iframe src="https://onedrive.live.com/embed?cid=154152893557B712&amp;resid=154152893557B712%212890&amp;authkey=AFSQaX-TNRFFeRA&amp;em=2&amp;wdAr=1.7777777777777777" width="1186px" height="691px" frameborder="0">This is an embedded <a target="_blank" href="https://office.com">Microsoft Office</a> presentation, powered by <a target="_blank" href="https://office.com/webapps">Office</a>.</iframe><!--kg-card-end: html--><p>In each of these series, I gave the introduction talk. I touched on product development as well. The audience greatly enjoyed my stories on history of electronics and original skunkworks (nighthawk, blackbird aeroplane design tradeoffs).</p><!--kg-card-begin: html--><iframe src="https://onedrive.live.com/embed?cid=154152893557B712&amp;resid=154152893557B712%212905&amp;authkey=AO1j-7kN9x3TReY&amp;em=2&amp;wdAr=1.7777777777777777" width="1186px" height="691px" frameborder="0">This is an embedded <a target="_blank" href="https://office.com">Microsoft Office</a> presentation, powered by <a target="_blank" href="https://office.com/webapps">Office</a>.</iframe><!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[A Jaffna Marriage Story, Jam-Fruit-Tree-style]]></title><description><![CDATA[<p>During the war, many Tamils escaped abroad. Several from my family. Usually boys of age around 20-25. They get in as refugees, work and send money home here. &#x200B;<br>&#x200B;<br>So, in their 28-30, when they settle and get refugee PR, they have gotta get married, right? You cant move</p>]]></description><link>https://aba-blog.xyz/jaffna-marriage-story/</link><guid isPermaLink="false">6199140734a827036b0506ee</guid><category><![CDATA[Thoughts]]></category><category><![CDATA[Stories]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Tue, 27 Apr 2021 00:00:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2022/01/jaffna-wedding.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2022/01/jaffna-wedding.jpg" alt="A Jaffna Marriage Story, Jam-Fruit-Tree-style"><p>During the war, many Tamils escaped abroad. Several from my family. Usually boys of age around 20-25. They get in as refugees, work and send money home here. &#x200B;<br>&#x200B;<br>So, in their 28-30, when they settle and get refugee PR, they have gotta get married, right? You cant move easily back and forth. And tickets are expensive. So the standard procedure was this:&#x200B;<br>&#x200B;<br>The family of the boy will find a girl of the same caste and good horoscope. Often, the boy doesn&apos;t even get to look at her picture. They arrange the marriage here. During the reception, the boy&apos;s mom wears the ring to the girl and forges her son&apos;s signature. Marriage is done! They send the girl abroad to live happily ever after with the boy, mostly on a fake passport / as a PR wife. &#x200B;</p><h3 id="this-was-a-widespread-practice-now-the-best-part">This was a widespread practice. Now, the best part</h3><p>Remember, these girls have never talked to their guys, probably never talked to any guy. Now, they have a lot of money and jewelry packed and are terrified of living alone abroad. So, naturally, many of them elope on the way. I mean, prior to the flight, they stay at a hotel in Colombo (with a family member), &#xA0;right? They run with the charming hotel guy. Which was also quite common.&#x200B;</p><p>So, when my uncle (mom&apos;s 2nd cousin, Canada) was at the right age, he told my mom:</p><blockquote>&quot;You find a girl for me. I don&apos;t even want to see her. But don&apos;t send her like this. I&apos;ll come there, marry her and take her here. Else she might run away with someone on the way&quot;&#x200B;</blockquote><h3 id="okay-what-is-jam-fruit-tree">Okay, what is Jam Fruit Tree?</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/the-jam-fruit-tree-book-review/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">The Jam Fruit Tree - Book Review</div><div class="kg-bookmark-description">One of the best novels I&#x2019;ve read in English! The &#x2018;chronicle&#x2019; start with an intensely sexual narrative and develops into a thoroughly detailed account of the Burgher community of Sri Lanka from 1930s to 1980s. Their history, culture, festivals, food, wedding, funeral, day to day life... it&#x2019;s all told&#x2026;</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt="A Jaffna Marriage Story, Jam-Fruit-Tree-style"><span class="kg-bookmark-author">Aba&apos;s Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://aba-blog.xyz/content/images/size/w2000/2021/11/100955219_10219786304639565_8158902497491025920_n.jpg" alt="A Jaffna Marriage Story, Jam-Fruit-Tree-style"></div></a></figure><h3 id="first-posted-on-facebook">First posted on Facebook:</h3><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="http://web.facebook.com/abarajithan11/posts/10222304020740894"><div class="kg-bookmark-content"><div class="kg-bookmark-title">Abarajithan Gn</div><div class="kg-bookmark-description">A Jaffna Marriage Story, Jam Fruit Tree style:&#x200B;&#x200B;During the war, many Tamils escaped abroad. Several from my family. Usually boys of age around 20-25. They get in as refugees, work and send money...</div><div class="kg-bookmark-metadata"><img class="kg-bookmark-icon" src="https://static.xx.fbcdn.net/rsrc.php/yv/r/B8BxsscfVBr.ico" alt="A Jaffna Marriage Story, Jam-Fruit-Tree-style"><span class="kg-bookmark-author">Facebook</span></div></div><div class="kg-bookmark-thumbnail"><img src="https://scontent.fcmb7-1.fna.fbcdn.net/v/t1.6435-1/124589615_10221159028996816_3199591677153576602_n.jpg?_nc_cat=105&amp;ccb=1-5&amp;_nc_sid=baafbc&amp;_nc_ohc=fDsPEnrpl8gAX9dQj-g&amp;_nc_ht=scontent.fcmb7-1.fna&amp;oh=00_AT8zRGU_0dWpwMyAynZM9UqQUqiXEOstNAWIxfKIaQ5CVw&amp;oe=6209AF9F" alt="A Jaffna Marriage Story, Jam-Fruit-Tree-style"></div></a></figure><p></p>]]></content:encoded></item><item><title><![CDATA[Lost Technologies: Few Possible Parallels]]></title><description><![CDATA[<p>Modern sagas like Lord of the Rings and Game of Thrones have so many implicit references to actual history, its fascinating. &#x200B;</p><h2 id="damascus-wootz-steel">Damascus / Wootz Steel</h2><p>Wootz steel was invented in a village in Tamil Nadu, before the 5th century BC. Alexander (356 BC) was super impressed by its unique qualities</p>]]></description><link>https://aba-blog.xyz/lost-technologies-fact-fiction/</link><guid isPermaLink="false">6199157034a827036b050711</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Sat, 20 Mar 2021 00:00:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/the-lord-of-the-rings-the-return-of-the-king.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/the-lord-of-the-rings-the-return-of-the-king.jpg" alt="Lost Technologies: Few Possible Parallels"><p>Modern sagas like Lord of the Rings and Game of Thrones have so many implicit references to actual history, its fascinating. &#x200B;</p><h2 id="damascus-wootz-steel">Damascus / Wootz Steel</h2><p>Wootz steel was invented in a village in Tamil Nadu, before the 5th century BC. Alexander (356 BC) was super impressed by its unique qualities such as toughness and resistance to shattering. Cheras brought it to Sri Lanka around 5th c. BC and it was then manufactured in Anuradhapura as well. &#x200B;<br>&#x200B;<br>The technology was kept secret and the steel was exported to Middle East as ingots, where it was forged into swords and called Damascus Steel. Wootz/Damascus steel is characterized by wavy lines on them, formed due to the process that was lost with time. Micheal Faraday was obsessed with it and tried his best to recreate it. That&apos;s basically the Valaryan steel, the lost steel technology with wavy lines... From GoT</p><figure class="kg-card kg-image-card"><img src="https://aba-blog.xyz/content/images/2021/11/damascus-class-768x768.jpg" class="kg-image" alt="Lost Technologies: Few Possible Parallels" loading="lazy" width="768" height="768" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/damascus-class-768x768.jpg 600w, https://aba-blog.xyz/content/images/2021/11/damascus-class-768x768.jpg 768w" sizes="(min-width: 720px) 720px"></figure><h3 id="byzantium">Byzantium</h3><p>Rome and Constantinople (Istanbul) were called the twin cities: &quot;Two eyes of the world&quot;. One at the rising sun and the other at the setting sun. The three concentric walls of Constantinople, one taller than the other, gleaming with white limestone, were famous around the world. And we have Minas Tirtih (White city of Gondor) and Minas Morgul.&#x200B;</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2021/11/the-lord-of-the-rings-the-return-of-the-king-1.jpg" class="kg-image" alt="Lost Technologies: Few Possible Parallels" loading="lazy" width="750" height="384" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/the-lord-of-the-rings-the-return-of-the-king-1.jpg 600w, https://aba-blog.xyz/content/images/2021/11/the-lord-of-the-rings-the-return-of-the-king-1.jpg 750w" sizes="(min-width: 720px) 720px"><figcaption>Minas Tirith</figcaption></figure><h3 id="greek-fire">Greek Fire</h3><p>When Arabs attacked, they captured the whole of the Byzantine empire easily. Romans didn&apos;t stand a chance. In a naval battle with Mu Avia, Emperor put his dress on a peasant and ran away to Constantinople. But Arabs couldn&apos;t capture Constantinople for a long time. Reason?&#x200B;</p><ol><li>Walls&#x200B;</li><li>Greek fire&#x200B;</li></ol><p>Greek fire was so powerful and was kept secret. It was invented by a refugee in Constantinople and used in key naval battles successfully. It could light the seas. Water couldn&apos;t quench it, by some accounts made it burn even brighter. Basically the wildfire in GoT.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://aba-blog.xyz/content/images/2021/11/Wildfire_on_Blackwater-1.jpg" class="kg-image" alt="Lost Technologies: Few Possible Parallels" loading="lazy" width="1280" height="720" srcset="https://aba-blog.xyz/content/images/size/w600/2021/11/Wildfire_on_Blackwater-1.jpg 600w, https://aba-blog.xyz/content/images/size/w1000/2021/11/Wildfire_on_Blackwater-1.jpg 1000w, https://aba-blog.xyz/content/images/2021/11/Wildfire_on_Blackwater-1.jpg 1280w" sizes="(min-width: 720px) 720px"><figcaption>Battle of the Blackwater: Game of Thrones</figcaption></figure><p>First posted on Facebook: <a href="https://web.facebook.com/abarajithan11/posts/10222057446896702">facebook.com/abarajithan11/posts/10222057446896702</a></p>]]></content:encoded></item><item><title><![CDATA[Self Driving on Mars: Computer vision on Xilinx FPGAs​]]></title><description><![CDATA[<p>Our final year project, my research area and my current job are centred around &quot;Computer Vision on FPGA&quot;, which is a relatively new field. I&apos;m super glad this field is gaining traction. Read on for some ExplainLikeImFive...&#x200B;</p><h3 id="why-computer-vision%E2%80%8B">Why computer vision?&#x200B;</h3><p>Perseverance, the $2.7</p>]]></description><link>https://aba-blog.xyz/self-driving-on-mars/</link><guid isPermaLink="false">61991a7f34a827036b050762</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Fri, 19 Feb 2021 00:00:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/mars-rover-pixl-sensor---nasa-.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/mars-rover-pixl-sensor---nasa-.jpg" alt="Self Driving on Mars: Computer vision on Xilinx FPGAs&#x200B;"><p>Our final year project, my research area and my current job are centred around &quot;Computer Vision on FPGA&quot;, which is a relatively new field. I&apos;m super glad this field is gaining traction. Read on for some ExplainLikeImFive...&#x200B;</p><h3 id="why-computer-vision%E2%80%8B">Why computer vision?&#x200B;</h3><p>Perseverance, the $2.7 billion, SUV sized rover that autonomously landed on Mars today does a lot of cool stuff. Since it takes over 5 minutes for signals to reach Mars, they cannot control it like flying an RC drone. Unlike previous rovers, this time the landing algorithm autonomously matched the images of the landing site for precise landing.&#x200B;<br>&#x200B;<br>Also, the rover is able to drive around by autonomously planning the path using CV algorithms and by processing other data. This makes it the fastest rover ever. With about 200m a day average speed.&#x200B;</p><h3 id="why-fpgas%E2%80%8B">Why FPGAs?&#x200B;</h3><p>Images are a bunch of large matrices. Computer vision is basically doing millions of repetitive calculations on them, to finally detect objects...etc. With normal CPUs with few big cores, this will take forever. So, we use GPUs, which have hundreds of tiny, dumb cores. But GPUs are power-hungry (gaming PCs serve a dual purpose as room heaters).&#x200B;<br>&#x200B;<br>Edge systems, like Mars rovers, are limited in power. So, RTL designers design custom digital circuits (verilog) to create hundreds of custom cores to parallelize their computations. Since we can optimize the cores to our particular need, the power consumption is super low. &#x200B;<br>&#x200B;<br>If you have enough demand, you can fabricate chips (ASIC) from these. When u make millions of chips (like those u put in mobile / computer), the per-chip price comes down to few dollars - super cheap. But since you don&apos;t make a million mars rovers, they put the custom circuit in a pre-built, reconfigurable canvas: an FPGA.<br>&#x200B;<br>Unlike Curiosity, Perseverance has a few Xilinx-made Virtex 5 FPGAs to perform all the CV needed for autonomous landing and self-driving. They can use the same chip to now do RGB images optimally, then change the circuit inside and do 3D images optimally. I&apos;m glad to see this field is booming.&#x200B;<br>&#x200B;<br>Hardcore info: <a href="https://www.fierceelectronics.com/electronics/nasa-mars-rover-perseverance-launches-thursday-to-find-evidence-life-red-planet?fbclid=IwAR32GQkm1smKsisFtgK1ia9cgoybw8rUmjn9sUjUDqiix1u9P8VJ7pHWEr0">fierceelectronics.com/electronics/nasa-mars-rover-perseverance-launches-thursday-to-find-evidence-life-red-planet</a></p><p>First posted on Facebook: <a href="https://web.facebook.com/abarajithan11/posts/10221852292447969">facebook.com/abarajithan11/posts/10221852292447969</a></p>]]></content:encoded></item><item><title><![CDATA[My Patriotism - Happy Independence Day]]></title><description><![CDATA[<p>I usually come under fire for criticizing the faults in the government, politics, society and culture of Sri Lanka as &quot;You never see the good side, you are never proud of your country, you think other countries are better&quot;. This might be because with whomever I discuss, I</p>]]></description><link>https://aba-blog.xyz/my-patriotism/</link><guid isPermaLink="false">61993d3c34a827036b05079f</guid><category><![CDATA[Thoughts]]></category><dc:creator><![CDATA[Abarajithan G]]></dc:creator><pubDate>Mon, 01 Feb 2021 00:00:00 GMT</pubDate><media:content url="https://aba-blog.xyz/content/images/2021/11/shutterstock_478550953-Flag-of-Sri-Lanka.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://aba-blog.xyz/content/images/2021/11/shutterstock_478550953-Flag-of-Sri-Lanka.jpg" alt="My Patriotism - Happy Independence Day"><p>I usually come under fire for criticizing the faults in the government, politics, society and culture of Sri Lanka as &quot;You never see the good side, you are never proud of your country, you think other countries are better&quot;. This might be because with whomever I discuss, I spend the time discussing our disagreements. I often play the devil&apos;s advocate on every topic. While just agreeing all the time might keep everyone happy, it adds no value to the conversation. &#x200B;</p><p>I find the following quote from Merl Chandana resonating with my worldview, not just about my country, but about everything that I love: my family, my friends, my university, the department... and so on.&#x200B;</p><blockquote>My patriotism has a very complicated definition&quot; this could have easily been something that came out of me. I feel very much the same. The only way to truly love our country - or love anything, really I guess - is to love it in all its complexity and chaos, but do everything we can to make it the best version of itself. Be accepting of our past, but ruthless about correcting the mistakes we made. Holding yourself and things you love to the highest imaginable standard is one of love&apos;s purest forms.&#x200B;</blockquote><p>Happy independence day everyone...</p><p>First posted on facebook: <a href="https://web.facebook.com/abarajithan11/posts/10221749308793442">facebook.com/abarajithan11/posts/10221749308793442</a></p><hr><p>To add to the original post, here&apos;s a message I got from a Brite, the German girl from the <a href="https://aba-blog.xyz/feel-lanka-2017/index.html">Feel Lanka project</a>, after 5 years of visiting Sri Lanka.</p><blockquote>And to be honest: I still love the Sri Lankan spirit<br>The people are always happy and help each other</blockquote><p>Her stay wasn&apos;t perfect. She actually went through some shit here, but she saw the best in people. We have things to be proud of, and to preserve here &#x2764;&#xFE0F;</p>]]></content:encoded></item></channel></rss>