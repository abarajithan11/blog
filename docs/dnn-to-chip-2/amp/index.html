<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>DNN → Chip Design [2/4: Golden Models]</title>

    <link rel="icon" href="../../favicon.png" type="image/png" />
    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="Aba&#x27;s Blog" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="DNN → Chip Design [2/4: Golden Models]" />
    <meta property="og:description" content="This is a series of articles [intro] outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken [arXiv paper].Golden Models are essential to hardware (FPGA/ASIC) development. They model the expected behavior of a chip using a high-level" />
    <meta property="og:url" content="https://aba-blog.xyz/dnn-to-chip-2/" />
    <meta property="og:image" content="https://aba-blog.xyz/content/images/size/w2000/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" />
    <meta property="article:published_time" content="2022-01-29T10:30:50.000Z" />
    <meta property="article:modified_time" content="2022-01-29T18:43:05.000Z" />
    <meta property="article:tag" content="Technical Projects" />
    
    <meta property="article:publisher" content="https://www.facebook.com/abarajithan11" />
    <meta property="article:author" content="https://www.facebook.com/abarajithan11" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="DNN → Chip Design [2/4: Golden Models]" />
    <meta name="twitter:description" content="This is a series of articles [intro] outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken [arXiv paper].Golden Models are essential to hardware (FPGA/ASIC) development. They model the expected behavior of a chip using a high-level" />
    <meta name="twitter:url" content="https://aba-blog.xyz/dnn-to-chip-2/" />
    <meta name="twitter:image" content="https://aba-blog.xyz/content/images/size/w2000/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Abarajithan G" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Technical Projects" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="1366" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Aba&#x27;s Blog",
        "url": "https://aba-blog.xyz/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://aba-blog.xyz/favicon.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Abarajithan G",
        "image": {
            "@type": "ImageObject",
            "url": "https://aba-blog.xyz/content/images/2021/11/im_large_vignette.jpg",
            "width": 1600,
            "height": 1600
        },
        "url": "https://aba-blog.xyz/author/aba/",
        "sameAs": [
            "https://www.linkedin.com/in/abarajithan11",
            "https://www.facebook.com/abarajithan11"
        ]
    },
    "headline": "DNN → Chip Design [2/4: Golden Models]",
    "url": "https://aba-blog.xyz/dnn-to-chip-2/",
    "datePublished": "2022-01-29T10:30:50.000Z",
    "dateModified": "2022-01-29T18:43:05.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://aba-blog.xyz/content/images/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg",
        "width": 2000,
        "height": 1366
    },
    "keywords": "Technical Projects",
    "description": "\n\n&gt; This is a series of articles [intro]\n[https://aba-blog.xyz/dnn-to-chip-1/index.html] outlining my workflow of 15\nsteps, which I developed over the past few years through building my own DNN\naccelerator: Kraken [arXiv paper] [https://arxiv.org/abs/2112.02793].\nGolden Models are essential to hardware (FPGA/ASIC) development. They model the\nexpected behavior of a chip using a high-level language, such that they can be\nbuilt relatively fast, with almost zero chance of error. The input and expect",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://aba-blog.xyz/"
    }
}
    </script>

    <meta name="generator" content="Ghost 4.32" />
    <link rel="alternate" type="application/rss+xml" title="Aba&#x27;s Blog" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content blockquote.kg-blockquote-alt {
        font-size: 1.2em;
        font-style: italic;
        line-height: 1.6em;
        text-align: center;
        color: #738a94;
        padding: 0.75em 3em 1.25em;
    }

    .post-content blockquote.kg-blockquote-alt::before {
        display: none;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 3px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-toggle-card-icon {
        display: none;
    }

    .kg-toggle-content {
        margin-top: 0.8rem;
    }

    .kg-product-card-container {
        background: transparent;
        padding: 20px;
        width: 100%;
        border-radius: 5px;
        box-shadow: inset 0 0 0 1px rgb(124 139 154 / 25%);
    }

    .kg-product-card-description p {
        margin-top: 1.5em;
    }

    .kg-product-card-description ul {
        margin-left: 24px;
    }

    .kg-product-card-title {
        font-size: 1.9rem;
        font-weight: 700;
    }

    .kg-product-card-rating-star {
        height: 28px;
        width: 20px;
        margin-right: 2px;
    }

    .kg-product-card-rating-star svg {
    width: 16px;
    height: 16px;
    fill: currentColor;
    opacity: 0.15;
    }

    .kg-product-card-rating-active.kg-product-card-rating-star svg {
    opacity: 1;
    }

    .kg-nft-card-container {
        position: relative;
        display: flex;
        flex: auto;
        flex-direction: column;
        text-decoration: none;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.4rem;
        font-weight: 400;
        box-shadow: 0 2px 6px -2px rgb(0 0 0 / 10%), 0 0 1px rgb(0 0 0 / 40%);
        width: 100%;
        max-width: 512px;
        color: #15212A;
        background: #fff;
        border-radius: 5px;
        transition: none;
        margin: 0 auto;
    }

    .kg-nft-metadata {
        padding: 2.0rem;
    }

    .kg-nft-image-container {
        position: relative;
    }

    .kg-nft-image {
        display: flex;
        border-radius: 5px 5px 0 0;
    }

    .kg-nft-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        gap: 20px;
    }

    .kg-nft-header h4.kg-nft-title {
        font-size: 1.9rem;
        font-weight: 700;
        margin: 0;
        color: #15212A;
    }

    .kg-nft-header amp-img {
        max-width: 114px;
        max-height: 26px;
    }

    .kg-nft-opensea-logo {
        margin-top: 2px;
        width: 100px;
    }

    .kg-nft-creator {
        font-family: inherit;
        color: #95A1AD;
    }

    .kg-nft-creator span {
        font-weight: 500;
        color: #15212A;
    }

    .kg-nft-card p.kg-nft-description {
        font-size: 1.4rem;
        line-height: 1.4em;
        margin: 2.0rem 0 0;
        color: #222;
    }

    .kg-button-card {
        display: flex;
        position: static;
        align-items: center;
        width: 100%;
        justify-content: center;
    }

    .kg-btn {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 2.0rem;
        height: 4.0rem;
        line-height: 4.0rem;
        font-size: 1.65rem;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
    }

    .kg-btn:hover {
        opacity: 0.85;
    }

    .kg-btn-accent {
        background-color: var(--ghost-accent-color, #1292EE);
        color: #fff;
    }

    .kg-callout-card {
        display: flex;
        padding: 20px 28px;
        border-radius: 3px;
    }

    .kg-callout-card-grey {
        background: rgba(124, 139, 154, 0.13);
    }

    .kg-callout-card-white {
        background: transparent;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-callout-card-blue {
        background: rgba(33, 172, 232, 0.12);
    }

    .kg-callout-card-green {
        background: rgba(52, 183, 67, 0.12);
    }

    .kg-callout-card-yellow {
        background: rgba(240, 165, 15, 0.13);
    }

    .kg-callout-card-red {
        background: rgba(209, 46, 46, 0.11);
    }

    .kg-callout-card-pink {
        background: rgba(225, 71, 174, 0.11);
    }

    .kg-callout-card-purple {
        background: rgba(135, 85, 236, 0.12);
    }

    .kg-callout-card-accent {
        background: var(--ghost-accent-color);
        color: #fff;
    }

    .kg-callout-card-accent a {
        color: #fff;
    }

    .kg-callout-emoji {
        padding-right: 16px;
        line-height: 1.3;
        font-size: 1.25em;
    }

    .kg-header-card {
        padding: 6em 3em;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        text-align: center;
    }

    .kg-header-card.kg-size-small {
        padding-top: 4em;
        padding-bottom: 4em;
    }

    .kg-header-card.kg-size-large {
        padding-top: 12em;
        padding-bottom: 12em;
    }

    .kg-header-card.kg-width-full {
        padding-left: 4em;
        padding-right: 4em;
    }

    .kg-header-card.kg-align-left {
        text-align: left;
        align-items: flex-start;
    }

    .kg-header-card.kg-style-dark {
        background: #15171a;
        color: #ffffff;
    }

    .kg-header-card.kg-style-light {
        color: #15171a;
        border: 1px solid rgba(124, 139, 154, 0.25);
        border-width: 1px 0;
    }

    .kg-header-card.kg-style-accent {
        background-color: var(--ghost-accent-color);
    }

    .kg-header-card.kg-style-image {
        background-color: #e7e7eb;
        background-size: cover;
        background-position: center center;
    }

    .kg-header-card h2 {
        font-size: 4em;
        font-weight: 700;
        line-height: 1.1em;
        margin: 0;
    }

    .kg-header-card h2 strong {
        font-weight: 800;
    }

    .kg-header-card.kg-size-small h2 {
        font-size: 3em;
    }

    .kg-header-card.kg-size-large h2 {
        font-size: 5em;
    }

    .kg-header-card h3 {
        font-size: 1.25em;
        font-weight: 500;
        line-height: 1.3em;
        margin: 0;
    }

    .kg-header-card h3 strong {
        font-weight: 600;
    }

    .kg-header-card.kg-size-small h3 {
        font-size: 1em;
    }

    .kg-header-card.kg-size-large h3 {
        font-size: 1.5em;
    }

    .kg-header-card:not(.kg-style-light) h2,
    .kg-header-card:not(.kg-style-light) h3 {
        color: #ffffff;
    }

    .kg-header-card a.kg-header-card-button {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 1.2em;
        height: 2.4em;
        line-height: 1em;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-size: 0.95em;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
        background-color: var(--ghost-accent-color);
        color: #ffffff;
        margin: 1.75em 0 0;
    }

    .kg-header-card a.kg-header-card-button:hover {
        opacity: 0.85;
    }

    .kg-header-card.kg-size-large a.kg-header-card-button {
        margin-top: 2em;
    }

    .kg-header-card.kg-size-small a.kg-header-card-button {
        margin-top: 1.5em;
    }

    .kg-header-card.kg-style-image a.kg-header-card-button,
    .kg-header-card.kg-style-dark a.kg-header-card-button {
        background: #ffffff;
        color: #15171a;
    }

    .kg-header-card.kg-style-accent a.kg-header-card-button {
        background: #ffffff;
        color: var(--ghost-accent-color);
    }

    .kg-audio-card {
        display: flex;
        width: 100%;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-audio-thumbnail {
        display: flex;
        justify-content: center;
        align-items: center;
        width: 80px;
        min-width: 80px;
        height: 80px;
        background: transparent;
        object-fit: cover;
        aspect-ratio: 1/1;
        border-radius: 3px 0 0 3px;
    }

    .kg-audio-thumbnail.placeholder {
        background: var(--ghost-accent-color);
    }

    .kg-audio-thumbnail.placeholder svg {
        width: 24px;
        height: 24px;
        fill: white;
    }

    .kg-audio-player-container {
        position: relative;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        width: 100%;
        --seek-before-width: 0%;
        --volume-before-width: 100%;
        --buffered-width: 0%;
    }

    .kg-audio-title {
        width: 100%;
        padding: 8px 12px 0;
        border: none;
        font-family: inherit;
        font-size: 1.1em;
        font-weight: 700;
        background: transparent;
    }

    .kg-audio-player {
        display: none;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #2861bd;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                <amp-img class="site-icon" src="https://aba-blog.xyz/content/images/2021/11/-Pngtree-electronic-chip-free-download_4712240.png" width="50" height="50" layout="fixed"></amp-img>
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">DNN → Chip Design [2/4: Golden Models]</h1>
                <section class="post-meta">
                    Abarajithan G -
                    <time class="post-date" datetime="2022-01-29">29 Jan 2022</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://aba-blog.xyz/content/images/2022/01/b6f87d_cec29645262b492f95a30c10edec1090_mv2.jpg" width="600" height="340" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p></p><blockquote>This is a series of articles <a href="https://aba-blog.xyz/dnn-to-chip-1/index.html">[intro]</a> outlining my workflow of 15 steps, which I developed over the past few years through building my own DNN accelerator: Kraken<a href="https://arxiv.org/abs/2112.02793"> [arXiv paper]</a>.</blockquote><p>Golden Models are essential to hardware (FPGA/ASIC) development. They model the expected behavior of a chip using a high-level language, such that they can be built relatively fast, with almost zero chance of error. The input and expected output test vectors for every RTL module are generated using them, and the simulation output from the testbench is compared against their 'gold standard.'</p><p>I first obtain pretrained DNNs from PyTorch / Tensorflow model zoo, analyze them, then load them into the custom DNN inference framework I have built with NumPy stack to ensure I fully understand each operation. I then generate test vectors from those golden models.</p><h2 id="steps">Steps:</h2><ol>
<li><strong>PyTorch/TensorFlow</strong>: Explore DNN models, quantize &amp; extract weights</li>
<li><strong>Golden Model in Python (NumPy stack)</strong>: Custom OOP framework, process the weights, convert to custom datatypes</li>
</ol>
<div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" data-kg-background-image><h2 class="kg-header-card-header">1. Tensorflow / PyTorch</h2></div><p>Tensorflow (Google) and PyTorch (Facebook) are the two competing open source libraries used to build, train, quantize and deploy modern deep neural networks. </p><p>Both frameworks provide high-level, user-friendly classes and functions such as Conv2D, model.fit() to build &amp; train networks. Each such high-level API is implemented using their own low-level tensor operations (matmul, einsum), which also can be used by the users. Those operations are implemented using their C++ backend, accelerated by high performant libraries like eigen and CUDA. Once we define the models using Python, the C++ code underneath pulls the load, making them fast as well as user-friendly.</p><h3 id="11-download-explore-pretrained-dnn-models">1.1. Download &amp; Explore Pretrained DNN Models</h3><p>As the first step, I obtained the pretrained models from either <a href="https://keras.io/api/applications/">Keras.Applications</a> or <a href="https://pytorch.org/serve/model_zoo.html">PyTorch Model Zoo</a>.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import tensorflow as tf
from tensorflow.keras.applications import VGG16

vgg16 = VGG16(
    include_top=True,
    weights="imagenet",
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation="softmax",
)

vgg16.save('saved_model/vgg16')
vgg16.summary()</code></pre><figcaption>Get VGG16 from TensorFlow zoo (Keras.Applications)</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)
model.eval()
torch.save(model, "alexnet.pt")</code></pre><figcaption>Get AlexNet from PyTorch Model Zoo</figcaption></figure><h3 id="12-build-models-retrain-if-needed-pytorch">1.2. <strong>Build Models &amp; </strong>Retrain if needed (PyTorch)</h3><p>PyTorch is more intuitive, pythonic and bliss to work with. I use it to build new models and train them if needed. </p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets
from torchvision import transforms as T
from torch.optim.lr_scheduler import StepLR

H = 28
N = 32
device = torch.device("cuda")

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(H**2, 32)
        self.fc2 = nn.Linear(32, 10)
    def forward(self, x):
        x = self.fc1(x)
        x = F.leaky_relu(x,0.01)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output
        
model = Net().to(device)</code></pre><figcaption>Building a simple fully connected network in PyTorch</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python">''' Create Data Loaders to efficiently pull data '''
transform = T.Compose([T.ToTensor(), T.Lambda(lambda x: torch.flatten(x))])
dataset1  = datasets.MNIST('../data', train=True, download=True, transform=transform)
dataset2  = datasets.MNIST('../data', train=False, transform=transform)

train_loader = torch.utils.data.DataLoader(dataset1,batch_size=N)
test_loader = torch.utils.data.DataLoader(dataset2,batch_size=N)

''' Functions to Test &amp; Train '''

def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for (data, target) in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        print(f'Train, epoch: {epoch}, loss: {loss.item():.6f}')

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    print(f'Test, loss: {test_loss}, acc: {100*correct/len(test_loader.dataset):.0f}')

optimizer = optim.Adadelta(model.parameters(), lr=1.0)
scheduler = StepLR(optimizer, step_size=1, gamma=0.7)

''' Train for 50 Epochs '''
for epoch in range(1, 51):
    train(model, device, train_loader, optimizer, epoch)
    test(model, device, test_loader)
    scheduler.step()

''' Save Trained Model '''
torch.save(model, "mnist_cnn.pt")</code></pre><figcaption>Training the model in PyTorch with MNIST dataset</figcaption></figure><h3 id="13-convert-torch-models-to-tensorflow">1.3. Convert Torch Models to Tensorflow</h3><p>However, the support for int8 quantization for PyTorch is still experimental. Therefore, for most of my work, I use pretrained models from Tensorflow, whose quantization library (TFLite) is much superior. </p><p>Some models, like AlexNet, are not found in Keras.Applications. Therefore, I load them from PyTorch Model Zoo and convert them to ONNX (the common open-source format) and then load them in Tensorflow.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">import torch
from PIL import Image
from torchvision import transforms

model = torch.load('alexnet.pt')

input_image = Image.open('dog.jpg')
preprocess = transforms.Compose([
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),
])
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0)

torch.onnx.export(model, input_batch, "alexnet.onnx",
		input_names=['input'], output_names=['output'])</code></pre><figcaption>Open the saved AlexNet model and export as ONNX</figcaption></figure><figure class="kg-card kg-code-card"><pre><code class="language-python"># First install onnx2keras with: pip install onnx onnx2keras
import onnx
from onnx2keras import onnx_to_keras
onnx_model = onnx.load("alexnet.onnx")
k_model = onnx_to_keras(onnx_model, ['input'], change_ordering=True)
k_model.save('saved_model/alexnet')</code></pre><figcaption>Convert ONNX model to Keras (Tensorflow)</figcaption></figure><h3 id="14-quantize-models-with-tensorflowlite">1.4. Quantize Models with TensorFlowLite</h3><p>Following is an example of loading a float32 model (VGG16) from tensorflow's savedmodel format (1.1), testing it, quantizing it to int8, and testing  &amp; saving the quantized network.</p><pre><code class="language-python">import tensorflow as tf
filenames = glob("dataset/*.jpg")

'''
LOAD AND TEST FLOAT32 MODEL
'''
prep_fn = tf.keras.applications.vgg16.preprocess_input
model = tf.keras.models.load_model(f'saved_model/vgg16')
h = model.input_shape[1]

import cv2
from glob import glob
import numpy as np
def representative_data_gen():
    for im_path in filenames:
        im = cv2.imread(im_path)
        im = cv2.resize(im, (h,h))
        im = im[None,:,:,::-1]
        im = prep_fn(im)
        im = tf.convert_to_tensor(im)
        yield [im]

images = list(representative_data_gen())

predictions = np.zeros((len(images),), dtype=int)
for i, image in enumerate(images):
    output = model(image[0])[0]
    predictions[i] = output.numpy().argmax()
print(predictions)

'''
CONVERT AND SAVE INT8 MODEL (STATIC QUANTIZATION)
'''
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
tflite_model_quant = converter.convert()

import pathlib
tflite_model_quant_file = pathlib.Path(f"tflite/vgg16.tflite")
tflite_model_quant_file.write_bytes(tflite_model_quant)

'''
LOAD AND TEST QUANTIZED MODEL
'''
interpreter = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()[0]
output_details = interpreter.get_output_details()[0]

images = list(representative_data_gen())
predictions = np.zeros((len(images),), dtype=int)

for i, image in enumerate(images):
    image = image[0]
    input_scale, input_zero_point = input_details["quantization"]
    image = image / input_scale + input_zero_point

    test_image = image.numpy().astype(input_details["dtype"])
    interpreter.set_tensor(input_details["index"], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details["index"])[0]
    predictions[i] = output.argmax()

print(predictions)</code></pre><h3 id="15-explore-model-architecture">1.5 Explore Model Architecture</h3><p><a href="https://netron.app/">Netron </a>is a great tool for opening tensorflow's 32-bit models (savedmodel), tflite's int8 models (tflite), pytorch models (pt), ONNX models, and more, to observe the architecture and tensor names.</p><figure class="kg-card kg-image-card"></figure><div class="kg-card kg-header-card kg-width-full kg-size-small kg-style-dark" data-kg-background-image><h2 class="kg-header-card-header">2. Golden Model</h2><h3 class="kg-header-card-subheader">Python (NumPy stack)</h3></div><p>After obtaining the pretrained model, I need to 100% understand what operations are involved and how they are applied as data flows through the network. The best way to do this is to re-do it myself from scratch and obtain exactly the same results. </p><h3 id="21-custom-quantization-scheme">2.1. Custom Quantization Scheme</h3><figure class="kg-card kg-image-card"></figure><h3 id="22-custom-inference-framework-oop-python">2.2. Custom Inference Framework (OOP, Python)</h3><p>For this, I built a custom framework in Python. It is structured like Keras with the following classes, inheriting as follows:</p><ul>
<li>MyModel</li>
<li>MyLayer
<ul>
<li>MyConv</li>
<li>MyLeakyReLU</li>
<li>MyMaxpool</li>
<li>MyConcat</li>
<li>MySpaceToDepth</li>
<li>MyFlatten</li>
</ul>
</li>
</ul>
<p>A MyModel object has a list of objects from MyLayer's children's classes. It's constructor extracts weights from tflite and sets them to the layers. A set of images can flow through the layers through a recursive call to the last layer. Following is the stripped-down version of the MyConv implementation.</p><figure class="kg-card kg-code-card"><pre><code class="language-python">class MyConv(MyLayer):
    def __init__(self,
                 weights_biases,
                 prev_layer=None,
                 bn_weights=None,
                 name='',
                 np_dtype=np.float64,
                 np_dtype_sum=np.float64,
                 np_dtype_conv_out=np.float64,
                 float_dtype=np.float64,
                 bits_conv_out=32,
                 quantize=False,
                 float_ieee=True):

        MyLayer.__init__(self,
                         name=name,
                         prev_layer=prev_layer,
                         np_dtype=np_dtype,
                         quantize=quantize,
                         float_ieee=float_ieee)
        self.np_dtype_sum = np_dtype_sum
        self.np_dtype_conv_out = np_dtype_conv_out
        self.float_dtype = float_dtype

        assert len(weights_biases[0].shape) == 4
        self.weights = weights_biases[0].astype(self.np_dtype)
        self.weights_flipped = np.flip(
            self.weights, [0, 1]).astype(self.np_dtype)
        self.kernel = self.weights.shape[0:2]
        self.in_ch_n = self.weights.shape[2]
        self.out_ch_n = self.weights.shape[3]
        self.biases = weights_biases[1].astype(self.np_dtype)

        self.fuse_bn(bn_weights)

        if self.quantize:
            self.clip_max = 2**(bits_conv_out-1)-1
            self.clip_min = -2**(bits_conv_out-1)

    def np_out(self, in_data):
        if self.quantize:
            in_data = self.in_data.copy().astype(self.np_dtype_sum)
            weights = self.weights.copy().astype(self.np_dtype_sum)

            self.np_out_data = self.conv2d_einsum(in_data, weights)

            self.np_out_data = np.clip(self.np_out_data, self.clip_min, self.clip_max)
            self.np_out_data = self.np_out_data.astype(self.np_dtype_conv_out)
        else:
            out = self.conv2d_einsum(self.in_data, self.weights)
            out += self.biases
            self.np_out_data = self.decode(self.encode(out))
        return self.np_out_data

    def fuse_bn(self, bn_weights, epsilon=0.001):
        self.gamma, self.beta, self.mean, self.variance = bn_weights
        self.epsilon = epsilon

        scale = self.gamma / np.sqrt(self.variance + self.epsilon)

        self.pure_weights = self.weights.copy()
        self.pure_biases = self.biases.copy()

        self.weights = self.weights * scale
        self.weights_flipped = np.flip(self.weights, [0, 1])
        self.biases = beta + scale * (self.biases - self.mean)

    @staticmethod
    def conv2d_einsum(img, kernel):
        pad_h = kernel.shape[0]//2
        pad_w = kernel.shape[1]//2

        out_batch = []
        for n in range(img.shape[0]):
            padding = ((pad_h, pad_h), (pad_w, pad_w), (0, 0))
            img_pad = np.pad(img[n], padding, 'constant')

            sub_shape = tuple(np.subtract(img_pad.shape, kernel.shape[0:-1])+1)
            shape = kernel.shape[0:-1] + sub_shape
            strd = np.lib.stride_tricks.as_strided
            submatrices = strd(img_pad,shape,img_pad.strides*2,writeable=False)

            out_batch += [np.einsum('ijkl,ijkmno-&gt;mnl', kernel, submatrices)]
        return np.array(out_batch)</code></pre><figcaption>stripped-down version of the MyConv implementation</figcaption></figure><h3 id="23-rebuilding-the-model-debugging">2.3. Rebuilding the model &amp; Debugging</h3><p>I then rebuild the model using the above framework, pass data and tweak things until I get the exact same output. That tells me I have understood all the operations going on inside the model.</p><figure class="kg-card kg-gallery-card kg-width-wide"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"></div><div class="kg-gallery-image"></div><div class="kg-gallery-image"></div></div></div></figure><p>Once I've understood the model inside-out, I start designing the hardware on the whiteboard.</p><h2 id="next">Next:</h2><figure class="kg-card kg-bookmark-card"><a class="kg-bookmark-container" href="https://aba-blog.xyz/dnn-to-chip-3/index.html"><div class="kg-bookmark-content"><div class="kg-bookmark-title">DNN → Chip Design [3/4: Digital Design]</div><div class="kg-bookmark-description">This is a series of articles outlining my workflow, which I developed over the past few years through building my own DNN accelerator: Kraken. Kraken Paper Whiteboard: Design hardware RTL Design: SystemVerilog/Verilog for the whiteboard designs Generate Test Vectors: using Python Notebooks Testbenc…</div><div class="kg-bookmark-metadata"><amp-img class="kg-bookmark-icon" src="https://aba-blog.xyz/favicon.png" alt width="70" height="70" layout="fixed"></amp-img><span class="kg-bookmark-author">Aba's Blog</span><span class="kg-bookmark-publisher">Abarajithan G</span></div></div><div class="kg-bookmark-thumbnail"><amp-img src="https://aba-blog.xyz/content/images/size/w2000/2022/01/Data-Flow-of-Inference-Engine-2-1.png" alt width="2000" height="1042" layout="responsive"></amp-img></div></a></figure>

            </section>

        </article>
    </main>
    <footer class="page-footer">
            <amp-img class="site-icon" src="https://aba-blog.xyz/content/images/2021/11/-Pngtree-electronic-chip-free-download_4712240.png" width="50" height="50" layout="fixed"></amp-img>
        <h3>Aba&#x27;s Blog</h3>
            <p>Tech - Travels - Thoughts</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>
